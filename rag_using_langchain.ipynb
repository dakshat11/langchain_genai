{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nG6m3Q9DBHV1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33R54QYjCMAJ"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F7iZIBmB8_T",
        "outputId": "79dacaf9-8646-431e-82b2-4eb168d96a55"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\daksh\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -q youtube-transcript-api langchain-community langchain-openai \\\n",
        "               faiss-cpu tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPcswe0tExci"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZZrs-ijCTYt"
      },
      "source": [
        "## Step 1a - Indexing (Document Ingestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "1p9AXZycFIH6",
        "outputId": "866503af-45c2-4788-9815-a5204a117109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FetchedTranscript(snippets=[FetchedTranscriptSnippet(text='Over the last few years, AI systems have become', start=3.46, duration=2.867), FetchedTranscriptSnippet(text='astonishingly good at turning text props into videos.', start=6.327, duration=3.293), FetchedTranscriptSnippet(text='At the core of how these models operate is a deep connection to physics.', start=10.24, duration=3.6), FetchedTranscriptSnippet(text='This generation of image and video models works using a process known as diffusion,', start=14.56, duration=4.664), FetchedTranscriptSnippet(text='which is remarkably equivalent to the Brownian motion we see as particles diffuse,', start=19.224, duration=4.664), FetchedTranscriptSnippet(text='but with time run backwards, and in high-dimensional space.', start=23.888, duration=3.372), FetchedTranscriptSnippet(text=\"As we'll see, this connection to physics is much more than a curiosity.\", start=28.2, duration=3.46), FetchedTranscriptSnippet(text='We get real algorithms out of the physics that we can use to generate images and videos.', start=31.88, duration=4.58), FetchedTranscriptSnippet(text='And this perspective will also give us some really', start=36.96, duration=2.181), FetchedTranscriptSnippet(text='nice intuitions for how these models work in practice.', start=39.141, duration=2.399), FetchedTranscriptSnippet(text=\"But before we dive into this connection, let's get hands-on with a real diffusion model.\", start=42.6, duration=4.28), FetchedTranscriptSnippet(text='While the best models are closed source, there are some compelling open source models.', start=47.76, duration=4.46), FetchedTranscriptSnippet(text='This video of an astronaut was generated by an open source model called WAN 2.1.', start=52.94, duration=4.44), FetchedTranscriptSnippet(text='We can add to our prompt and have our astronaut hold a flag,', start=57.9, duration=3.46), FetchedTranscriptSnippet(text='hold a laptop, or hold a meeting.', start=61.36, duration=1.96), FetchedTranscriptSnippet(text='If we cut down our prompt to just an astronaut, we get this.', start=64.14, duration=3.4), FetchedTranscriptSnippet(text='And if we cut down our prompt to nothing, we interestingly', start=68.08, duration=2.562), FetchedTranscriptSnippet(text='still get this video of a woman.', start=70.642, duration=1.458), FetchedTranscriptSnippet(text=\"If we dig into our WAN model's source code, we'll find that the video\", start=73.14, duration=3.825), FetchedTranscriptSnippet(text='generation process begins with this call to a random number generator.', start=76.965, duration=3.935), FetchedTranscriptSnippet(text='Creating a video where the pixel intensity values are chosen randomly.', start=81.42, duration=3.36), FetchedTranscriptSnippet(text=\"Here's what it looks like.\", start=85.46, duration=1.1), FetchedTranscriptSnippet(text='From here, this pure noise video is passed into a transformer.', start=87.46, duration=3.34), FetchedTranscriptSnippet(text='This is the same type of AI model used by large language models, like ChatGPT.', start=91.22, duration=4.46), FetchedTranscriptSnippet(text='But instead of outputting text, this transformer', start=96.3, duration=2.78), FetchedTranscriptSnippet(text='outputs another video that now looks like this.', start=99.08, duration=2.78), FetchedTranscriptSnippet(text='Still mostly noise, but with some hints of structure.', start=102.6, duration=2.6), FetchedTranscriptSnippet(text='This new video is added to our pure noise video,', start=105.78, duration=2.656), FetchedTranscriptSnippet(text='and then passed back into the model again, producing a third video that looks like this.', start=108.436, duration=4.924), FetchedTranscriptSnippet(text='This process is repeated again and again.', start=114.3, duration=1.96), FetchedTranscriptSnippet(text=\"Here's what the video looks like after 5 iterations, 10, 20, 30, 40, and finally 50.\", start=116.84, duration=7.44), FetchedTranscriptSnippet(text='Step by step, our transformer shapes pure noise into incredibly realistic video.', start=125.84, duration=5.12), FetchedTranscriptSnippet(text='But what exactly is the connection to Brownian motion here?', start=131.84, duration=3.44), FetchedTranscriptSnippet(text='And how is our model able to use text input so expressively', start=135.86, duration=3.284), FetchedTranscriptSnippet(text='to shape noise into what our prompt describes?', start=139.144, duration=2.616), FetchedTranscriptSnippet(text=\"In this video, we'll impact diffusion models in 3 parts.\", start=142.82, duration=2.86), FetchedTranscriptSnippet(text=\"First we'll look at a 2021 OpenAI paper and model called CLIP.\", start=146.7, duration=3.46), FetchedTranscriptSnippet(text=\"As we'll see, CLIP is really two models, a language model and a vision model,\", start=150.84, duration=4.028), FetchedTranscriptSnippet(text='that are trained using a clever learning objective that allows them to', start=154.868, duration=3.715), FetchedTranscriptSnippet(text='learn this really powerful shared space between words and pictures.', start=158.583, duration=3.557), FetchedTranscriptSnippet(text='Experimenting with this space will help us get a feel for', start=163.64, duration=2.893), FetchedTranscriptSnippet(text='the high dimensional spaces that diffusion models operate in.', start=166.533, duration=3.147), FetchedTranscriptSnippet(text='But learning a shared representation is not enough to generate images.', start=170.18, duration=3.1), FetchedTranscriptSnippet(text=\"From here we'll look at the diffusion process itself.\", start=173.98, duration=2.18), FetchedTranscriptSnippet(text='At a high level, diffusion models are trained to remove noise from images or videos.', start=176.94, duration=4.56), FetchedTranscriptSnippet(text='However, if you dig into the landmark papers in the field,', start=182.34, duration=2.631), FetchedTranscriptSnippet(text=\"you'll find that this naive understanding of diffusion really doesn't hold\", start=184.971, duration=3.403), FetchedTranscriptSnippet(text='up in practice.', start=188.374, duration=0.726), FetchedTranscriptSnippet(text=\"In this section we'll dig into the connection between\", start=189.78, duration=2.45), FetchedTranscriptSnippet(text='diffusion models and diffusion processes in physics.', start=192.23, duration=2.45), FetchedTranscriptSnippet(text='This connection will help us understand how these models really work in practice and', start=195.44, duration=4.21), FetchedTranscriptSnippet(text='give us some powerful theory for dramatically speeding up image and video generation.', start=199.65, duration=4.31), FetchedTranscriptSnippet(text=\"Finally, we'll bring these worlds together and see how approaches\", start=205.32, duration=3.211), FetchedTranscriptSnippet(text='like CLIP are combined with diffusion models to condition and guide', start=208.531, duration=3.36), FetchedTranscriptSnippet(text='the generation process towards the videos we ask for in our prompts.', start=211.891, duration=3.409), FetchedTranscriptSnippet(text='2020 was a landmark year for language modeling.', start=217.26, duration=2.76), FetchedTranscriptSnippet(text=\"New results in neural scaling laws and OpenAI's\", start=220.8, duration=3.109), FetchedTranscriptSnippet(text='GPT-3 showed that bigger really was better.', start=223.909, duration=2.911), FetchedTranscriptSnippet(text='Massive models trained on massive datasets had', start=227.76, duration=2.465), FetchedTranscriptSnippet(text=\"capabilities that simply didn't exist in smaller models.\", start=230.225, duration=3.055), FetchedTranscriptSnippet(text=\"It didn't take long for researchers to apply similar ideas to images.\", start=233.72, duration=3.74), FetchedTranscriptSnippet(text='In February 2021, a team at OpenAI released a new model architecture called CLIP,', start=238.1, duration=4.754), FetchedTranscriptSnippet(text='trained on a dataset of 400 million image and caption pairs scraped from the internet.', start=242.854, duration=5.106), FetchedTranscriptSnippet(text='CLIP is composed of two models, one that processes text and one that processes images.', start=248.74, duration=4.46), FetchedTranscriptSnippet(text='The output of each of these models is a vector of length 512,', start=254.26, duration=3.23), FetchedTranscriptSnippet(text='and the central idea is that the vectors for a given image and its captions', start=257.49, duration=4.024), FetchedTranscriptSnippet(text='should be similar.', start=261.514, duration=1.006), FetchedTranscriptSnippet(text='To achieve this, the OpenAI team developed a clever training approach.', start=263.18, duration=4.04), FetchedTranscriptSnippet(text='Given a batch of image-caption pairs, for example our batch could contain', start=268.42, duration=3.885), FetchedTranscriptSnippet(text='a picture of a cat, a dog, and me, with the captions a photo of a cat,', start=272.305, duration=3.778), FetchedTranscriptSnippet(text='a photo of a dog, and a photo of a man, we then pass our three images', start=276.083, duration=3.725), FetchedTranscriptSnippet(text='into our image model, and our three captions into our text model.', start=279.808, duration=3.512), FetchedTranscriptSnippet(text='We now have three image vectors and three text vectors,', start=284.26, duration=2.963), FetchedTranscriptSnippet(text='and we would like the vectors for the matching image-caption pairs to be similar.', start=287.223, duration=4.417), FetchedTranscriptSnippet(text='The clever idea from here is to make use of the similarity not', start=292.12, duration=2.959), FetchedTranscriptSnippet(text='just between the corresponding images and captions,', start=295.079, duration=2.482), FetchedTranscriptSnippet(text='but between all image-caption pairs in the batch when training our models.', start=297.561, duration=3.579), FetchedTranscriptSnippet(text='If we arrange our image vectors as the columns of a matrix,', start=301.72, duration=3.083), FetchedTranscriptSnippet(text='and our text vectors as the rows, the pairs of vectors along', start=304.803, duration=3.187), FetchedTranscriptSnippet(text='the diagonal of our matrix correspond to matching images and captions.', start=307.99, duration=3.71), FetchedTranscriptSnippet(text='And all the pairs off-diagonal are non-matching images and captions.', start=311.84, duration=3.74), FetchedTranscriptSnippet(text='The CLIP training objective seeks to maximize the similarity between', start=316.24, duration=3.721), FetchedTranscriptSnippet(text='corresponding image-caption pairs, while simultaneously minimizing', start=319.961, duration=3.666), FetchedTranscriptSnippet(text='the similarity between non-corresponding image-caption pairs.', start=323.627, duration=3.393), FetchedTranscriptSnippet(text='The C in CLIP stands for contrastive, because the model learns', start=328.1, duration=3.587), FetchedTranscriptSnippet(text='to contrast matching and non-matching image-caption pairs.', start=331.687, duration=3.413), FetchedTranscriptSnippet(text='The CLIP algorithm measures similarity between', start=336.24, duration=2.179), FetchedTranscriptSnippet(text='vectors using a metric called cosine similarity.', start=338.419, duration=2.321), FetchedTranscriptSnippet(text='Geometrically, we can think of each of these vectors as', start=341.74, duration=2.452), FetchedTranscriptSnippet(text='pointing in some direction in high-dimensional space.', start=344.192, duration=2.408), FetchedTranscriptSnippet(text='Cosine similarity measures the cosine of the angle between our vectors in this space.', start=347.58, duration=4.48), FetchedTranscriptSnippet(text='So if our text and image vector point in the same direction,', start=353.14, duration=3.102), FetchedTranscriptSnippet(text='the angle between our vectors will be zero, resulting in a maximum value for our cosine', start=356.242, duration=4.549), FetchedTranscriptSnippet(text='similarity score of 1.', start=360.791, duration=1.189), FetchedTranscriptSnippet(text='So the image and text models that make up CLIP are trained to maximize the', start=363.3, duration=4.024), FetchedTranscriptSnippet(text='alignment of related images and captions in this shared high-dimensional space,', start=367.324, duration=4.35), FetchedTranscriptSnippet(text='while minimizing the alignment between unrelated images and captions.', start=371.674, duration=3.806), FetchedTranscriptSnippet(text='The learned geometry of this shared vector space,', start=376.6, duration=2.801), FetchedTranscriptSnippet(text='known as a latent or embedding space, has some really interesting properties.', start=379.401, duration=4.459), FetchedTranscriptSnippet(text='If I take two pictures of myself, one not wearing a hat and one wearing a hat,', start=384.52, duration=4.452), FetchedTranscriptSnippet(text='and pass both of these into our CLIP image model,', start=388.972, duration=2.854), FetchedTranscriptSnippet(text='we get two vectors in our embedding space.', start=391.826, duration=2.454), FetchedTranscriptSnippet(text='Now if I take the vector corresponding to me wearing a hat,', start=395.38, duration=3.004), FetchedTranscriptSnippet(text='and subtract the vector of me not wearing a hat,', start=398.384, duration=2.495), FetchedTranscriptSnippet(text='we get a new vector in our embedding space.', start=400.879, duration=2.241), FetchedTranscriptSnippet(text='Now what text might this new vector correspond to?', start=403.64, duration=2.9), FetchedTranscriptSnippet(text='Mathematically we took the difference of me wearing a hat and me not wearing a hat.', start=408.04, duration=3.52), FetchedTranscriptSnippet(text='We can search for corresponding text by passing a bunch of different', start=412.06, duration=3.72), FetchedTranscriptSnippet(text='words into our text encoder, and for each computing the cosine similarity', start=415.78, duration=4.049), FetchedTranscriptSnippet(text='between our newly computed difference vector and the text vector.', start=419.829, duration=3.611), FetchedTranscriptSnippet(text='Testing a set of a few hundred common words, the top ranked match with', start=424.32, duration=4.415), FetchedTranscriptSnippet(text='a similarity of 0.165 is the word hat, followed by cap and helmet.', start=428.735, duration=4.225), FetchedTranscriptSnippet(text='This is a remarkable result.', start=433.78, duration=1.88), FetchedTranscriptSnippet(text=\"The learned geometry of CLIP's embedding space allows us to operate\", start=436.68, duration=3.853), FetchedTranscriptSnippet(text='mathematically on the pure ideas or concepts in our images and text,', start=440.533, duration=3.967), FetchedTranscriptSnippet(text='translating the differences in the content of our images,', start=444.5, duration=3.335), FetchedTranscriptSnippet(text=\"like if there's a hat or not, into a literal distance between vectors in\", start=447.835, duration=4.198), FetchedTranscriptSnippet(text='our embedding space.', start=452.033, duration=1.207), FetchedTranscriptSnippet(text='The OpenAI team showed that CLIP could produce very impressive image classification', start=453.94, duration=4.507), FetchedTranscriptSnippet(text='results by simply passing an image into our image encoder,', start=458.447, duration=3.204), FetchedTranscriptSnippet(text='and then comparing the resulting vector to a set of possible captions,', start=461.651, duration=3.855), FetchedTranscriptSnippet(text='one for each label that could be assigned to the image,', start=465.506, duration=3.041), FetchedTranscriptSnippet(text='and classifying the image with whatever label resulted in the highest cosine similarity.', start=468.547, duration=4.833), FetchedTranscriptSnippet(text='So techniques like CLIP give us a powerful shared representation of image and text,', start=474.6, duration=4.898), FetchedTranscriptSnippet(text='a kind of vector space of pure ideas.', start=479.498, duration=2.242), FetchedTranscriptSnippet(text='However, our CLIP models only go one direction.', start=482.68, duration=2.72), FetchedTranscriptSnippet(text='We can only map image and text to our shared embedding space.', start=486.1, duration=3.06), FetchedTranscriptSnippet(text='We have no way of generating images and text from our embedding vectors.', start=490.0, duration=3.48), FetchedTranscriptSnippet(text='2020 turned out not only to be a transformative year for language modeling.', start=495.46, duration=4.1), FetchedTranscriptSnippet(text='A few weeks after the GPT-3 paper came out, a team at Berkeley published a', start=500.32, duration=4.68), FetchedTranscriptSnippet(text='paper called Denoising Diffusion Probabilistic Models, now known as DDPM.', start=505.0, duration=4.68), FetchedTranscriptSnippet(text='The paper showed for the first time that it was possible to', start=510.26, duration=3.446), FetchedTranscriptSnippet(text='generate very high quality images using a diffusion process,', start=513.706, duration=3.563), FetchedTranscriptSnippet(text='where pure noise is transformed step by step into realistic images.', start=517.269, duration=3.971), FetchedTranscriptSnippet(text='The core idea behind diffusion models is pretty straightforward.', start=522.419, duration=2.881), FetchedTranscriptSnippet(text='We take a set of training images and add noise to each', start=526.24, duration=3.107), FetchedTranscriptSnippet(text='image step by step until the image is completely destroyed.', start=529.347, duration=3.453), FetchedTranscriptSnippet(text='From here we train a neural network to reverse this process.', start=533.94, duration=2.9), FetchedTranscriptSnippet(text='When I first learned about diffusion models, I assumed that the', start=537.68, duration=3.229), FetchedTranscriptSnippet(text='models would be trained to remove noise a single step at a time.', start=540.909, duration=3.331), FetchedTranscriptSnippet(text='Our model would be trained to predict the image in step 1 given the noisier image in step', start=544.74, duration=4.616), FetchedTranscriptSnippet(text='2, trained to predict the image in step 2 given the noisier image in step 3, and so on.', start=549.356, duration=4.564), FetchedTranscriptSnippet(text='When it came time to generate an image, we would pass pure noise into our model,', start=554.88, duration=4.098), FetchedTranscriptSnippet(text='take its output and pass it back into its input again and again,', start=558.978, duration=3.33), FetchedTranscriptSnippet(text='and after enough steps we would have a nice image.', start=562.308, duration=2.612), FetchedTranscriptSnippet(text='Now, it turns out that this naive approach to', start=565.76, duration=2.482), FetchedTranscriptSnippet(text='building a diffusion model really does not work well.', start=568.242, duration=2.978), FetchedTranscriptSnippet(text='Virtually no modern models work like this.', start=571.98, duration=2.2), FetchedTranscriptSnippet(text=\"These are the training and image generation algorithms from the Berkeley team's paper.\", start=575.12, duration=4.16), FetchedTranscriptSnippet(text=\"The notation is a bit dense, but there's some key details we can pull out\", start=580.1, duration=3.485), FetchedTranscriptSnippet(text='that will help us understand what it takes to make these models really work.', start=583.585, duration=3.675), FetchedTranscriptSnippet(text='The first thing that surprised me is that the team added random noise', start=587.94, duration=3.505), FetchedTranscriptSnippet(text='to images not just during training, but also during image generation.', start=591.445, duration=3.555), FetchedTranscriptSnippet(text='Algorithm 2 tells us that when generating new images, at each step,', start=595.92, duration=3.967), FetchedTranscriptSnippet(text='after our neural network predicts a less noisy image,', start=599.887, duration=3.197), FetchedTranscriptSnippet(text='we need to add random noise to this image before passing it back into our model.', start=603.084, duration=4.796), FetchedTranscriptSnippet(text='This added noise turns out to matter a lot in practice.', start=608.6, duration=2.74), FetchedTranscriptSnippet(text=\"If we take a popular diffusion model like stable diffusion 2 and use the Berkeley team's\", start=612.38, duration=5.109), FetchedTranscriptSnippet(text='image generation approach, known as DDPM sampling, we can get some really nice images.', start=617.489, duration=5.051), FetchedTranscriptSnippet(text=\"Here's the image we get when prompting the model with this prompt,\", start=623.16, duration=3.187), FetchedTranscriptSnippet(text='asking for a tree in the desert.', start=626.347, duration=1.593), FetchedTranscriptSnippet(text='Now, if we remove the line of code that adds noise at each step', start=628.68, duration=3.624), FetchedTranscriptSnippet(text='of the generation process, we end up with a tiny sad blurry tree.', start=632.304, duration=3.796), FetchedTranscriptSnippet(text='How is it that adding random noise while generating images leads to better quality,', start=637.04, duration=4.527), FetchedTranscriptSnippet(text='sharper images?', start=641.567, duration=0.873), FetchedTranscriptSnippet(text=\"The second thing that surprised me when I encountered the Berkeley team's approach was\", start=643.6, duration=4.093), FetchedTranscriptSnippet(text=\"that the team wasn't training models to reverse a single step in the noise addition\", start=647.693, duration=3.999), FetchedTranscriptSnippet(text='process.', start=651.692, duration=0.428), FetchedTranscriptSnippet(text='Instead, the team takes an initial clean image, which they call X0,', start=652.82, duration=3.683), FetchedTranscriptSnippet(text='and adds scaled random noise to the image, which they call epsilon.', start=656.503, duration=3.737), FetchedTranscriptSnippet(text='And from here, they train the model to predict the', start=660.54, duration=2.48), FetchedTranscriptSnippet(text='total noise that was added to the original image.', start=663.02, duration=2.48), FetchedTranscriptSnippet(text='So the team is effectively asking the model to skip all the', start=666.3, duration=3.222), FetchedTranscriptSnippet(text='intermediate steps and make a prediction about the original image.', start=669.522, duration=3.658), FetchedTranscriptSnippet(text='Intuitively, this learning task seems much more difficult to me', start=674.06, duration=2.943), FetchedTranscriptSnippet(text='than just learning to make a noisy image slightly less noisy.', start=677.003, duration=2.897), FetchedTranscriptSnippet(text=\"The Berkeley team's paper and approach was a landmark\", start=681.04, duration=2.761), FetchedTranscriptSnippet(text='result that put diffusion on the map.', start=683.801, duration=1.979), FetchedTranscriptSnippet(text='Why does adding random noise while generating images', start=686.82, duration=2.574), FetchedTranscriptSnippet(text='and training the model like this work so well?', start=689.394, duration=2.326), FetchedTranscriptSnippet(text='The DDPM paper draws on some fairly complex theory to arrive at these algorithms.', start=693.0, duration=4.56), FetchedTranscriptSnippet(text=\"I'll include a link to a great tutorial in the\", start=698.12, duration=2.004), FetchedTranscriptSnippet(text='description if you want to dig deeper into the theory.', start=700.124, duration=2.396), FetchedTranscriptSnippet(text=\"Happily, it turns out that there's a different but mathematically equivalent\", start=703.38, duration=3.614), FetchedTranscriptSnippet(text='way of understanding what diffusion models are really learning that we can', start=706.994, duration=3.567), FetchedTranscriptSnippet(text='use to get a visual and intuitive sense for why the DDPM algorithms work so well.', start=710.561, duration=3.899), FetchedTranscriptSnippet(text='The key will be thinking of diffusion models as learning a time-varying vector field.', start=715.28, duration=4.32), FetchedTranscriptSnippet(text='This perspective also leads to a more general approach called flow-based models,', start=720.22, duration=4.35), FetchedTranscriptSnippet(text='which have become very popular recently.', start=724.57, duration=2.23), FetchedTranscriptSnippet(text='To see how diffusion models learn this time-varying vector field,', start=727.64, duration=3.467), FetchedTranscriptSnippet(text=\"let's temporarily simplify our learning problem.\", start=731.107, duration=2.613), FetchedTranscriptSnippet(text='One way to think about an image is as a point in high-dimensional space,', start=734.64, duration=3.756), FetchedTranscriptSnippet(text='where the intensity value of each pixel controls the position of the point in each', start=738.396, duration=4.33), FetchedTranscriptSnippet(text='dimension.', start=742.726, duration=0.574), FetchedTranscriptSnippet(text='If we reduce the size of our images to only two pixels,', start=744.26, duration=2.812), FetchedTranscriptSnippet(text='we can visualize the distribution of our images by plotting the pixel intensity', start=747.072, duration=4.09), FetchedTranscriptSnippet(text='value of our first pixel on the x-axis of scatterplot and the pixel intensity of', start=751.162, duration=4.142), FetchedTranscriptSnippet(text='our second pixel on the y-axis.', start=755.304, duration=1.636), FetchedTranscriptSnippet(text='So an image with a black first pixel and a white second pixel', start=758.12, duration=3.291), FetchedTranscriptSnippet(text='would show up at x equals zero and y equals one on our scatterplot.', start=761.411, duration=3.669), FetchedTranscriptSnippet(text='And an all-white image would be at one, one, and so on.', start=765.66, duration=2.84), FetchedTranscriptSnippet(text='Now, real images have a very specific structure in this high-dimensional space.', start=769.38, duration=4.06), FetchedTranscriptSnippet(text=\"Let's create some structure for our points in our lower\", start=773.9, duration=2.388), FetchedTranscriptSnippet(text='two-dimensional space for our diffusion model to learn.', start=776.288, duration=2.432), FetchedTranscriptSnippet(text=\"The exact structure we choose doesn't matter too much at this point.\", start=779.38, duration=3.08), FetchedTranscriptSnippet(text=\"Let's start with a spiral shape like this.\", start=782.46, duration=2.52), FetchedTranscriptSnippet(text='The core idea of diffusion models, adding more and more noise to an', start=785.94, duration=3.421), FetchedTranscriptSnippet(text='image and then training a neural network to reverse this process,', start=789.361, duration=3.369), FetchedTranscriptSnippet(text='looks really interesting from the perspective of our 2D toy data.', start=792.73, duration=3.37), FetchedTranscriptSnippet(text=\"When we add random noise to an image, we're effectively\", start=797.2, duration=3.001), FetchedTranscriptSnippet(text=\"changing each pixel's value by a random amount.\", start=800.201, duration=2.619), FetchedTranscriptSnippet(text='In our toy 2D dataset, where the coordinates of a point correspond', start=803.3, duration=3.702), FetchedTranscriptSnippet(text=\"to that image's pixel intensity values, adding random noise is\", start=807.002, duration=3.533), FetchedTranscriptSnippet(text='equivalent to taking a step in a randomly chosen direction.', start=810.535, duration=3.365), FetchedTranscriptSnippet(text='As we add more and more noise to our image, our point goes on a random walk.', start=814.38, duration=3.8), FetchedTranscriptSnippet(text='This process is equivalent to the Brownian motion that drives diffusion', start=818.9, duration=3.571), FetchedTranscriptSnippet(text='processes in physics and is where diffusion models get their name.', start=822.471, duration=3.369), FetchedTranscriptSnippet(text=\"From here, it's pretty wild to think about what we're asking our diffusion model to do.\", start=826.76, duration=3.9), FetchedTranscriptSnippet(text='Our model will see many different random walks from various starting points in our', start=831.7, duration=4.342), FetchedTranscriptSnippet(text=\"dataset, and we're effectively asking our model to reverse the clock,\", start=836.042, duration=3.707), FetchedTranscriptSnippet(text='removing noise from our images by letting it play these diffusion processes backwards,', start=839.749, duration=4.607), FetchedTranscriptSnippet(text='starting our points from random locations and recovering the original structure of', start=844.356, duration=4.396), FetchedTranscriptSnippet(text='our dataset.', start=848.752, duration=0.688), FetchedTranscriptSnippet(text='How can our model learn to reverse these random walks?', start=850.24, duration=3.14), FetchedTranscriptSnippet(text='If we consider the specific point at the end of this 100-step random walk,', start=854.4, duration=4.126), FetchedTranscriptSnippet(text='in our naive diffusion modeling approach, where we ask our model to denoise images a', start=858.526, duration=4.74), FetchedTranscriptSnippet(text='single step at a time, this is equivalent to giving our model the coordinates of the', start=863.266, duration=4.74), FetchedTranscriptSnippet(text='final 100th point in our walk, and asking our model to predict the coordinates of our', start=868.006, duration=4.796), FetchedTranscriptSnippet(text='point at the 99th step.', start=872.802, duration=1.338), FetchedTranscriptSnippet(text='Although the direction of our 100th step is chosen randomly,', start=874.94, duration=3.086), FetchedTranscriptSnippet(text='there will be some signal in aggregate for our model to learn from here.', start=878.026, duration=3.754), FetchedTranscriptSnippet(text='Given enough training points, we expect many diffusion paths to go through', start=882.74, duration=4.256), FetchedTranscriptSnippet(text='this neighborhood, and on average our points will be diffusing away from', start=886.996, duration=4.198), FetchedTranscriptSnippet(text='our starting spiral, so our model can learn to point back towards our spiral.', start=891.194, duration=4.486), FetchedTranscriptSnippet(text=\"We can now see why the Berkeley team's training objective works so well.\", start=896.78, duration=3.42), FetchedTranscriptSnippet(text='Instead of training the model to remove noise from images one step at a time,', start=901.3, duration=3.799), FetchedTranscriptSnippet(text='this would correspond to predicting the coordinates of the 99th step given the 100th,', start=905.099, duration=4.242), FetchedTranscriptSnippet(text='the team instead trained the model to predict the total noise added across the entire', start=909.341, duration=4.243), FetchedTranscriptSnippet(text='walk.', start=913.584, duration=0.296), FetchedTranscriptSnippet(text='On our plot, this is the vector pointing from our 100th', start=914.64, duration=2.523), FetchedTranscriptSnippet(text='step back to the original starting point of the walk.', start=917.163, duration=2.477), FetchedTranscriptSnippet(text='It turns out that we can prove that learning to predict the noise added', start=920.32, duration=3.623), FetchedTranscriptSnippet(text='in the final step of our walk is mathematically equivalent to learning', start=923.943, duration=3.623), FetchedTranscriptSnippet(text='to predict the total noise added, divided by the number of steps taken.', start=927.566, duration=3.674), FetchedTranscriptSnippet(text='This means that when our model learns to reverse a single step,', start=932.22, duration=3.507), FetchedTranscriptSnippet(text='although our training data is noisy, we expect our model to ultimately learn to', start=935.727, duration=4.453), FetchedTranscriptSnippet(text='point back towards x0.', start=940.18, duration=1.28), FetchedTranscriptSnippet(text='By instead training our model to directly predict the vector pointing back towards x0,', start=942.58, duration=4.759), FetchedTranscriptSnippet(text=\"we're significantly reducing the variance of our training examples,\", start=947.339, duration=3.763), FetchedTranscriptSnippet(text='allowing our model to learn much more efficiently,', start=951.102, duration=2.822), FetchedTranscriptSnippet(text='without actually changing our underlying learning objective.', start=953.924, duration=3.376), FetchedTranscriptSnippet(text='So for each point in our space, our model learns the direction', start=958.58, duration=3.186), FetchedTranscriptSnippet(text='pointing back towards the original data distribution.', start=961.766, duration=2.774), FetchedTranscriptSnippet(text='This is also known as a score function, and the intuition here is that', start=965.28, duration=3.955), FetchedTranscriptSnippet(text='the score function points us towards more likely, less noisy data.', start=969.235, duration=3.785), FetchedTranscriptSnippet(text='Now, in practice, these learned directions depend', start=974.34, duration=2.752), FetchedTranscriptSnippet(text='heavily on how much noise we add to our original data.', start=977.092, duration=3.088), FetchedTranscriptSnippet(text='After 100 steps, most of our points are far from their starting points,', start=980.86, duration=3.69), FetchedTranscriptSnippet(text='so our model learns to move these points back in the general direction of our spiral.', start=984.55, duration=4.47), FetchedTranscriptSnippet(text='However, if we train our model on examples after only one diffusion step,', start=989.94, duration=3.798), FetchedTranscriptSnippet(text='we end up with a much more nuanced vector field,', start=993.738, duration=2.549), FetchedTranscriptSnippet(text='pointing to the fine structure of our spiral.', start=996.287, duration=2.393), FetchedTranscriptSnippet(text='There turns out to be a clever solution to this problem.', start=999.68, duration=2.66), FetchedTranscriptSnippet(text='Instead of just passing in the coordinates of our point into our model,', start=1002.88, duration=3.72), FetchedTranscriptSnippet(text=\"which we'll write here as a function f, we can also pass in a time\", start=1006.6, duration=3.51), FetchedTranscriptSnippet(text='variable that corresponds to the number of steps taken in our random walk.', start=1010.11, duration=3.93), FetchedTranscriptSnippet(text='If we set t equal to 1 at our 100th step, then t would equal 0.99 at our 99th step,', start=1014.86, duration=5.492), FetchedTranscriptSnippet(text='and so on.', start=1020.352, duration=0.728), FetchedTranscriptSnippet(text='Conditioning our models on time like this turns out to be essential in practice,', start=1021.64, duration=4.382), FetchedTranscriptSnippet(text='allowing our model to learn coarse vector fields for large values of t,', start=1026.022, duration=3.944), FetchedTranscriptSnippet(text='and very refined structures as t approaches 0.', start=1029.966, duration=2.574), FetchedTranscriptSnippet(text='After training, we can watch the time evolution of our model.', start=1033.619, duration=3.261), FetchedTranscriptSnippet(text='We see this really interesting behavior as t approaches 0.4.', start=1038.4, duration=3.6), FetchedTranscriptSnippet(text='Our learned vector field suddenly transitions,', start=1043.02, duration=2.168), FetchedTranscriptSnippet(text='from pointing towards the center of the spiral to pointing towards the spiral itself.', start=1045.188, duration=4.052), FetchedTranscriptSnippet(text='It feels like a phase change.', start=1049.9, duration=1.64), FetchedTranscriptSnippet(text=\"We're now in a great position to resolve the final mystery of the DDPM paper.\", start=1053.48, duration=4.32), FetchedTranscriptSnippet(text='How is it that adding random noise at each step while', start=1058.5, duration=2.924), FetchedTranscriptSnippet(text='generating images leads to better quality, sharper images?', start=1061.424, duration=3.256), FetchedTranscriptSnippet(text=\"Let's follow the path of a single point guided by the DDPM image generation algorithm.\", start=1065.54, duration=4.88), FetchedTranscriptSnippet(text='On our 2D dataset, generating an image is equivalent to starting', start=1071.08, duration=3.461), FetchedTranscriptSnippet(text='at a random location and working our way back to our spiral.', start=1074.541, duration=3.299), FetchedTranscriptSnippet(text='Starting at a randomly chosen location of x equals minus 1.6 and y equals 1.8,', start=1079.08, duration=4.499), FetchedTranscriptSnippet(text=\"our model's vector field points us back towards our spiral.\", start=1083.579, duration=3.461), FetchedTranscriptSnippet(text='Following the DDPM algorithm, we take a small step in the direction returned by our', start=1088.28, duration=4.782), FetchedTranscriptSnippet(text='model, and add scaled random noise, which effectively moves our point in a random', start=1093.062, duration=4.724), FetchedTranscriptSnippet(text='direction.', start=1097.786, duration=0.634), FetchedTranscriptSnippet(text=\"We'll color the steps driven by our diffusion model in blue, and our random steps in gray.\", start=1099.04, duration=4.34), FetchedTranscriptSnippet(text='Note that the scale of the random step may seem large, but following our DDPM algorithm,', start=1104.12, duration=4.781), FetchedTranscriptSnippet(text='the size of our random steps will come down as we progress.', start=1108.901, duration=3.259), FetchedTranscriptSnippet(text='Repeating this process for 64 steps, our particle jumps around', start=1113.0, duration=3.798), FetchedTranscriptSnippet(text='quite a bit due to both our learned vector field changing and our random noise steps,', start=1116.798, duration=5.268), FetchedTranscriptSnippet(text='but ultimately lands nicely on our spiral.', start=1122.066, duration=2.634), FetchedTranscriptSnippet(text='Repeating this process for a point cloud of 256 points,', start=1125.94, duration=3.329), FetchedTranscriptSnippet(text='our reverse diffusion process starts out looking like absolute chaos,', start=1129.269, duration=4.236), FetchedTranscriptSnippet(text='but does converge nicely, with most points landing on our spiral.', start=1133.505, duration=3.995), FetchedTranscriptSnippet(text='Now, what happens if we remove the noise addition steps?', start=1138.66, duration=3.3), FetchedTranscriptSnippet(text='Running our reverse diffusion process again without the random noise step,', start=1143.3, duration=3.715), FetchedTranscriptSnippet(text='all of our points quickly move to the center of our spiral,', start=1147.015, duration=3.012), FetchedTranscriptSnippet(text='and then make their way towards a single inside edge of the spiral.', start=1150.027, duration=3.413), FetchedTranscriptSnippet(text='This result can help us make sense of why we saw a sad', start=1154.54, duration=2.984), FetchedTranscriptSnippet(text='blurry tree earlier when we removed this random noise step.', start=1157.524, duration=3.316), FetchedTranscriptSnippet(text='Instead of capturing our full spiral distribution,', start=1161.64, duration=2.674), FetchedTranscriptSnippet(text='as we did when we included a noise step, all of our generated points end up close to', start=1164.314, duration=4.547), FetchedTranscriptSnippet(text='the center or average of our spiral.', start=1168.861, duration=1.979), FetchedTranscriptSnippet(text='In the space of images, averages look blurry.', start=1172.54, duration=2.9), FetchedTranscriptSnippet(text='Conceptually, we can imagine different parts of our spiral', start=1176.42, duration=2.78), FetchedTranscriptSnippet(text='corresponding to different images of trees in the desert.', start=1179.2, duration=2.78), FetchedTranscriptSnippet(text='And when we remove the random noise steps from our generation process,', start=1182.62, duration=3.598), FetchedTranscriptSnippet(text='our generated images end up in the center or average of these images,', start=1186.218, duration=3.597), FetchedTranscriptSnippet(text='which looks like a blurry mess.', start=1189.815, duration=1.645), FetchedTranscriptSnippet(text='Now, note that the analogy between our toy dataset and', start=1192.44, duration=2.675), FetchedTranscriptSnippet(text='high dimensional image dataset breaks down a bit here.', start=1195.115, duration=2.725), FetchedTranscriptSnippet(text='If all the points on our spiral correspond to realistic images,', start=1198.68, duration=3.492), FetchedTranscriptSnippet(text='since our generated points do still end up landing on our 2D spiral,', start=1202.172, duration=3.825), FetchedTranscriptSnippet(text='we would expect these generated points to still look like real images,', start=1205.997, duration=3.936), FetchedTranscriptSnippet(text='but likely with less diversity than we would want.', start=1209.933, duration=2.827), FetchedTranscriptSnippet(text='However, in the high dimensional space of images,', start=1213.76, duration=2.645), FetchedTranscriptSnippet(text=\"it appears that our image generation process doesn't quite make\", start=1216.405, duration=3.455), FetchedTranscriptSnippet(text='it to the manifold of realistic images, resulting in a blurry non-realistic image.', start=1219.86, duration=4.48), FetchedTranscriptSnippet(text='This prediction of the average is not a coincidence.', start=1225.4, duration=2.74), FetchedTranscriptSnippet(text='It turns out that we can show mathematically that our model', start=1229.12, duration=2.873), FetchedTranscriptSnippet(text='learns to point to the mean or average of our dataset,', start=1231.993, duration=2.678), FetchedTranscriptSnippet(text='conditioned on our input point and the time in our diffusion process.', start=1234.671, duration=3.409), FetchedTranscriptSnippet(text='One way to arrive at this result is to show that given the noise we add in our forward', start=1239.38, duration=4.507), FetchedTranscriptSnippet(text='process is Gaussian, for sufficiently small step sizes our reverse process will also', start=1243.887, duration=4.454), FetchedTranscriptSnippet(text='follow a Gaussian distribution, where our model actually learns the mean of this', start=1248.341, duration=4.245), FetchedTranscriptSnippet(text='distribution.', start=1252.586, duration=0.734), FetchedTranscriptSnippet(text='Since our model just predicts the mean of our normal distribution,', start=1254.92, duration=3.591), FetchedTranscriptSnippet(text='to actually sample from this distribution, we need to add zero mean', start=1258.511, duration=3.699), FetchedTranscriptSnippet(text=\"Gaussian noise to our model's predicted value,\", start=1262.21, duration=2.557), FetchedTranscriptSnippet(text='which is precisely what the DDPM image generation process does when we', start=1264.767, duration=3.863), FetchedTranscriptSnippet(text='add random noise after each step.', start=1268.63, duration=1.85), FetchedTranscriptSnippet(text='We can see this mean learning behavior most clearly early in our reverse diffusion', start=1271.94, duration=4.38), FetchedTranscriptSnippet(text='process, when t is close to 1 and our training points are far from our spiral.', start=1276.32, duration=4.22), FetchedTranscriptSnippet(text=\"Our model's learned vector field points towards the center or average of our dataset.\", start=1281.32, duration=3.8), FetchedTranscriptSnippet(text='So adding random noise during image generation falls nicely out of theory,', start=1286.22, duration=3.569), FetchedTranscriptSnippet(text='and in practice prevents all our points from landing near the center or average of', start=1289.789, duration=4.004), FetchedTranscriptSnippet(text='our dataset.', start=1293.793, duration=0.627), FetchedTranscriptSnippet(text='The DDPM paper put diffusion models on the map as a viable method of generating images,', start=1295.84, duration=4.859), FetchedTranscriptSnippet(text='but the diffusion approach did not immediately see widespread adoption.', start=1300.699, duration=4.021), FetchedTranscriptSnippet(text='A key issue with the DDPM approach at the time was the high compute demands of', start=1305.16, duration=4.211), FetchedTranscriptSnippet(text='the large number of steps required to generate high quality images,', start=1309.371, duration=3.671), FetchedTranscriptSnippet(text='since each step required a complete pass through a potentially very large neural network.', start=1313.042, duration=4.858), FetchedTranscriptSnippet(text=\"A few months later, a pair of papers from teams at Stanford and Google showed that it's\", start=1318.8, duration=4.537), FetchedTranscriptSnippet(text='remarkably possible to generate high quality images without actually adding random', start=1323.337, duration=4.329), FetchedTranscriptSnippet(text='noise during the generation process, significantly reducing the number of steps required.', start=1327.666, duration=4.694), FetchedTranscriptSnippet(text=\"The DDPM image generation process we've been looking at can be expressed using a\", start=1333.02, duration=4.074), FetchedTranscriptSnippet(text='special type of differential equation known as a stochastic differential equation.', start=1337.094, duration=4.226), FetchedTranscriptSnippet(text=\"This first term represents the motion of our point driven by our model's vector field,\", start=1342.16, duration=4.231), FetchedTranscriptSnippet(text='and the second term represents the random motions of our point.', start=1346.391, duration=3.149), FetchedTranscriptSnippet(text='Adding these terms together, we get the overall motion of our point at each step, dx.', start=1350.46, duration=4.0), FetchedTranscriptSnippet(text='From here, we can consider how the distribution of all of our points evolves over time,', start=1355.78, duration=4.35), FetchedTranscriptSnippet(text='where the motion of each point is governed by this stochastic differential equation.', start=1360.13, duration=4.25), FetchedTranscriptSnippet(text='This problem has been well studied in physics.', start=1365.58, duration=2.16), FetchedTranscriptSnippet(text='Using a key result from statistical mechanics known as the Fokker-Planck equation,', start=1368.44, duration=4.502), FetchedTranscriptSnippet(text=\"the Google Brain team showed that there's another differential equation,\", start=1372.942, duration=4.009), FetchedTranscriptSnippet(text='this time an ordinary differential equation with no random component,', start=1376.951, duration=3.843), FetchedTranscriptSnippet(text='that results in the same exact final distribution of points as our stochastic', start=1380.794, duration=4.283), FetchedTranscriptSnippet(text='differential equation.', start=1385.077, duration=1.263), FetchedTranscriptSnippet(text=\"This result gives us a new algorithm for generating images using our model's\", start=1387.92, duration=4.285), FetchedTranscriptSnippet(text='learned vector fields that does not require taking random steps along the way.', start=1392.205, duration=4.455), FetchedTranscriptSnippet(text='Exactly how our ordinary differential equation maps', start=1397.68, duration=2.619), FetchedTranscriptSnippet(text='to an image generation algorithm is a bit technical.', start=1400.299, duration=2.721), FetchedTranscriptSnippet(text=\"I'll leave a link to a tutorial in the description.\", start=1403.5, duration=2.02), FetchedTranscriptSnippet(text='The key result here though, is that we end up with something that looks very', start=1406.12, duration=4.19), FetchedTranscriptSnippet(text='similar to our DDPM image generation process,', start=1410.31, duration=2.536), FetchedTranscriptSnippet(text='but without the random noise addition at each step,', start=1412.846, duration=2.866), FetchedTranscriptSnippet(text='and with a new scaling for the sizes of steps that we take.', start=1415.712, duration=3.308), FetchedTranscriptSnippet(text='This approach is generally known as DDIM.', start=1419.7, duration=2.54), FetchedTranscriptSnippet(text='The scaling of our step sizes, and especially how these step sizes vary', start=1423.62, duration=3.735), FetchedTranscriptSnippet(text='throughout a reverse diffusion process, matters a lot in practice.', start=1427.355, duration=3.525), FetchedTranscriptSnippet(text='When we just removed the random noise steps from our DDPM generation algorithm earlier,', start=1432.2, duration=4.841), FetchedTranscriptSnippet(text='all of our points ended up near the mean of our data,', start=1437.041, duration=3.005), FetchedTranscriptSnippet(text='and we saw blurry results for our generated images.', start=1440.046, duration=2.894), FetchedTranscriptSnippet(text='Switching to our DDIM approach, we now have smaller scaling for our step', start=1443.96, duration=4.354), FetchedTranscriptSnippet(text='sizes that allow our trajectories to better follow the contour lines of', start=1448.314, duration=4.353), FetchedTranscriptSnippet(text='our vector field, and land nicely on the correct spiral distribution.', start=1452.667, duration=4.233), FetchedTranscriptSnippet(text='And applying our DDIM algorithm to our tree in the desert example,', start=1458.0, duration=3.553), FetchedTranscriptSnippet(text=\"we're now able to generate nice results.\", start=1461.553, duration=2.207), FetchedTranscriptSnippet(text='Comparing to our original DDPM algorithm that required random steps,', start=1464.86, duration=4.207), FetchedTranscriptSnippet(text='DDIM remarkably does not require any changes to model training,', start=1469.067, duration=3.959), FetchedTranscriptSnippet(text='but is able to generate high quality images in significantly fewer steps,', start=1473.026, duration=4.578), FetchedTranscriptSnippet(text='completely deterministically.', start=1477.604, duration=1.856), FetchedTranscriptSnippet(text='Note that the theory does not tell us that our individual images or', start=1480.48, duration=3.714), FetchedTranscriptSnippet(text='points on our spiral will be the same, but instead that our final', start=1484.194, duration=3.658), FetchedTranscriptSnippet(text='distribution of points or images will be the same,', start=1487.852, duration=2.827), FetchedTranscriptSnippet(text='regardless of whether we use our stochastic DDPM algorithm or our', start=1490.679, duration=3.658), FetchedTranscriptSnippet(text='deterministic DDIM algorithm.', start=1494.337, duration=1.663), FetchedTranscriptSnippet(text='The WAN model we saw earlier uses a generalization of DDIM called flow matching.', start=1496.8, duration=5.18), FetchedTranscriptSnippet(text='By early 2021, it was clear that diffusion models were capable of generating', start=1503.64, duration=4.218), FetchedTranscriptSnippet(text='high quality images, and thanks to image generation methods like DDIM,', start=1507.858, duration=3.94), FetchedTranscriptSnippet(text='it was possible to generate these images without using enormous amounts of compute.', start=1511.798, duration=4.662), FetchedTranscriptSnippet(text='However, our ability to steer the diffusion process', start=1517.18, duration=3.006), FetchedTranscriptSnippet(text='using text prompts was still very limited.', start=1520.186, duration=2.534), FetchedTranscriptSnippet(text='Earlier, we saw how CLIP was able to learn a powerful shared representation', start=1523.38, duration=4.29), FetchedTranscriptSnippet(text='of images and text by concurrently training image and text encoder models.', start=1527.67, duration=4.29), FetchedTranscriptSnippet(text='However, these models only go one way, converting text or images into embedding vectors.', start=1532.88, duration=5.02), FetchedTranscriptSnippet(text='These two problems potentially fit together in a really interesting way.', start=1538.78, duration=3.64), FetchedTranscriptSnippet(text='Diffusion models are able to potentially reverse the CLIP image encoder,', start=1543.46, duration=3.98), FetchedTranscriptSnippet(text='generating high quality images, and the output vector of the CLIP text encoder', start=1547.44, duration=4.366), FetchedTranscriptSnippet(text='could be used to guide our diffusion models toward the images or videos that we want.', start=1551.806, duration=4.754), FetchedTranscriptSnippet(text='So the high level idea here is that we could pass in a prompt into the CLIP text', start=1557.6, duration=4.251), FetchedTranscriptSnippet(text='encoder to generate an embedding vector, and use this embedding vector to steer', start=1561.851, duration=4.251), FetchedTranscriptSnippet(text='the diffusion process towards the image or video of what our prompt describes.', start=1566.102, duration=4.198), FetchedTranscriptSnippet(text='A team at OpenAI did exactly this in 2022.', start=1571.32, duration=3.08), FetchedTranscriptSnippet(text='Using image and caption pairs to train a diffusion model to invert the CLIP image encoder.', start=1575.1, duration=4.92), FetchedTranscriptSnippet(text='Their approach yielded an incredible level of prompt adherence,', start=1581.52, duration=3.284), FetchedTranscriptSnippet(text='capturing an unprecedented level of detail from the input text.', start=1584.804, duration=3.336), FetchedTranscriptSnippet(text='The team called their method unCLIP, but their', start=1589.2, duration=2.333), FetchedTranscriptSnippet(text='model is better known by its commercial name, DALI2.', start=1591.533, duration=2.687), FetchedTranscriptSnippet(text='But how do we actually use the embedding vectors', start=1595.52, duration=2.405), FetchedTranscriptSnippet(text='for models like CLIP to steer the diffusion process?', start=1597.925, duration=2.655), FetchedTranscriptSnippet(text='One option is to simply pass our text vector as another input into our diffusion model,', start=1601.26, duration=4.859), FetchedTranscriptSnippet(text='and train as we normally would to remove noise.', start=1606.119, duration=2.681), FetchedTranscriptSnippet(text='If we train our diffusion model using image and caption pairs,', start=1609.96, duration=3.187), FetchedTranscriptSnippet(text='as the OpenAI team did, the idea here is that the model will learn to', start=1613.147, duration=3.598), FetchedTranscriptSnippet(text='use the text information to more accurately remove noise from images,', start=1616.745, duration=3.597), FetchedTranscriptSnippet(text=\"since it now has more context about the image that it's learning to denoise.\", start=1620.342, duration=3.958), FetchedTranscriptSnippet(text='This technique is called conditioning.', start=1625.48, duration=1.52), FetchedTranscriptSnippet(text='We used a similar approach earlier, when we conditioned our toy diffusion', start=1627.68, duration=3.759), FetchedTranscriptSnippet(text='model on the number of time steps elapsed in the diffusion process,', start=1631.439, duration=3.501), FetchedTranscriptSnippet(text='allowing the model to learn coarse structure for large values of t,', start=1634.94, duration=3.501), FetchedTranscriptSnippet(text='and finer structures as our training samples get closer to our original spiral.', start=1638.441, duration=4.119), FetchedTranscriptSnippet(text='Interestingly, there turns out to be a variety of ways', start=1643.72, duration=2.783), FetchedTranscriptSnippet(text='we can pass in the text vector into our diffusion model.', start=1646.503, duration=2.937), FetchedTranscriptSnippet(text='Some approaches use a mechanism called cross-attention', start=1650.7, duration=2.712), FetchedTranscriptSnippet(text='to couple image and text information.', start=1653.412, duration=1.908), FetchedTranscriptSnippet(text='Other approaches simply add or append the embedded text vector to our diffusion', start=1655.92, duration=4.165), FetchedTranscriptSnippet(text=\"model's input, and some approaches pass in text information in multiple ways at once.\", start=1660.085, duration=4.535), FetchedTranscriptSnippet(text='Now it turns out that conditioning alone is not enough to achieve', start=1665.9, duration=3.346), FetchedTranscriptSnippet(text='the level of prompt adherence that we see in models like DALI2.', start=1669.246, duration=3.294), FetchedTranscriptSnippet(text=\"If we take the stable diffusion tree in the desert example we've been experimenting with,\", start=1673.56, duration=4.802), FetchedTranscriptSnippet(text='and only condition our model with our text inputs,', start=1678.362, duration=2.752), FetchedTranscriptSnippet(text='the model no longer gives us everything we ask for.', start=1681.114, duration=2.806), FetchedTranscriptSnippet(text='We get a shadow in a desert, but no tree.', start=1684.54, duration=2.54), FetchedTranscriptSnippet(text='Note that stable diffusion was developed by a team at Heidelberg University', start=1688.42, duration=3.789), FetchedTranscriptSnippet(text='around the same time as DALI2, and works in a similar way, but is open source.', start=1692.209, duration=3.991), FetchedTranscriptSnippet(text=\"It turns out that there's one more powerful idea that\", start=1697.44, duration=2.63), FetchedTranscriptSnippet(text='we need to effectively steer our diffusion models.', start=1700.07, duration=2.53), FetchedTranscriptSnippet(text='We can see this idea in action by returning to our toy dataset one last time.', start=1703.5, duration=3.84), FetchedTranscriptSnippet(text='If our overall spiral corresponds to realistic images,', start=1708.24, duration=2.712), FetchedTranscriptSnippet(text='then different sections of our spiral may correspond to different types of images.', start=1710.952, duration=4.168), FetchedTranscriptSnippet(text=\"Let's say this inner part is images of people, this middle part is images of dogs,\", start=1715.92, duration=4.194), FetchedTranscriptSnippet(text='and this outer part is different images of cats.', start=1720.114, duration=2.506), FetchedTranscriptSnippet(text=\"Now let's train the same diffusion model we trained earlier,\", start=1723.34, duration=3.079), FetchedTranscriptSnippet(text='but in addition to passing in our starting coordinates and the', start=1726.419, duration=3.232), FetchedTranscriptSnippet(text=\"time of our diffusion process, we'll also pass in the points class.\", start=1729.651, duration=3.489), FetchedTranscriptSnippet(text='Person, cat, or dog.', start=1733.94, duration=1.12), FetchedTranscriptSnippet(text='This extra signal should allow our model to steer points to', start=1735.82, duration=3.023), FetchedTranscriptSnippet(text='the right sections of our spiral, based on each points class.', start=1738.843, duration=3.177), FetchedTranscriptSnippet(text='Running our generation process, after assigning person, dog, or cat labels to each point,', start=1743.2, duration=4.854), FetchedTranscriptSnippet(text=\"we see that we're able to recover the overall structure of our dataset,\", start=1748.054, duration=3.927), FetchedTranscriptSnippet(text='but the fit is not great, and we see some confusion here between people and dog images.', start=1751.981, duration=4.799), FetchedTranscriptSnippet(text=\"Part of the problem here is that we're asking our model to simultaneously learn to point\", start=1758.24, duration=4.643), FetchedTranscriptSnippet(text='to our overall spiral of realistic images, and toward specific classes on our spiral.', start=1762.883, duration=4.537), FetchedTranscriptSnippet(text='If we consider this cat point for example, it starts off heading', start=1768.32, duration=3.552), FetchedTranscriptSnippet(text='towards the center of our spiral, and as our class conditioned vector', start=1771.872, duration=3.885), FetchedTranscriptSnippet(text='field shifts to point towards a cat region of our spiral,', start=1775.757, duration=3.219), FetchedTranscriptSnippet(text=\"our point moves towards this part of the spiral, but it doesn't quite make it.\", start=1778.976, duration=4.384), FetchedTranscriptSnippet(text='The modeling task of generally matching our overall spiral has overpowered', start=1784.1, duration=3.828), FetchedTranscriptSnippet(text=\"our model's ability to move our point in the direction of a specific class.\", start=1787.928, duration=3.932), FetchedTranscriptSnippet(text='Now, is there a way to decouple and maybe even control these two factors?', start=1793.26, duration=3.92), FetchedTranscriptSnippet(text='Remarkably, it turns out that we can.', start=1798.02, duration=1.5), FetchedTranscriptSnippet(text='The trick is to leverage the differences between the unconditional model that is not', start=1800.56, duration=3.937), FetchedTranscriptSnippet(text='trained on a specific class, and a model that is conditioned on specific classes.', start=1804.497, duration=3.843), FetchedTranscriptSnippet(text='We could do this by training two separate models,', start=1809.42, duration=2.378), FetchedTranscriptSnippet(text=\"but in practice it's more efficient to just leave out the class information for a\", start=1811.798, duration=3.98), FetchedTranscriptSnippet(text='subset of our training examples.', start=1815.778, duration=1.602), FetchedTranscriptSnippet(text='We now have the option of effectively passing in no class or text', start=1818.02, duration=3.408), FetchedTranscriptSnippet(text='information into our model, and getting back a vector field that', start=1821.428, duration=3.407), FetchedTranscriptSnippet(text='points towards our data in general, not towards any specific class.', start=1824.835, duration=3.565), FetchedTranscriptSnippet(text='We can visualize these two vector fields together.', start=1829.2, duration=2.5), FetchedTranscriptSnippet(text=\"Here the gray vectors show our diffusion model points when we don't pass in any class\", start=1832.44, duration=4.047), FetchedTranscriptSnippet(text='information, and these yellow vectors show when our model is conditioned on the cat', start=1836.487, duration=4.0), FetchedTranscriptSnippet(text='class.', start=1840.487, duration=0.333), FetchedTranscriptSnippet(text='For large values of our diffusion time variable when our training', start=1842.0, duration=3.355), FetchedTranscriptSnippet(text='data is far from our spiral, our two vector fields basically point', start=1845.355, duration=3.458), FetchedTranscriptSnippet(text='in the same direction, roughly towards the average of our spiral.', start=1848.813, duration=3.407), FetchedTranscriptSnippet(text='But as time approaches zero, our vector fields diverge,', start=1853.0, duration=3.177), FetchedTranscriptSnippet(text='with our cat conditioned vector field pointing more towards the outer cat', start=1856.177, duration=4.274), FetchedTranscriptSnippet(text='portion of our spiral.', start=1860.451, duration=1.329), FetchedTranscriptSnippet(text='Now that we have these two separate directions,', start=1862.76, duration=2.184), FetchedTranscriptSnippet(text='we can use their differences to push our points more in the direction', start=1864.944, duration=3.254), FetchedTranscriptSnippet(text='of the class we want.', start=1868.198, duration=1.022), FetchedTranscriptSnippet(text='Specifically, we take our yellow class conditioned', start=1870.66, duration=2.772), FetchedTranscriptSnippet(text='vector and subtract our gray unconditioned vector.', start=1873.432, duration=2.828), FetchedTranscriptSnippet(text='This gives us a new vector pointing from the tip of our', start=1876.32, duration=2.364), FetchedTranscriptSnippet(text='unconditioned vector to the tip of our conditioned vector.', start=1878.684, duration=2.536), FetchedTranscriptSnippet(text='The idea from here is that this direction should point more in the direction of our', start=1882.16, duration=4.165), FetchedTranscriptSnippet(text=\"cat examples, now that we've removed the direction generally pointing towards our data.\", start=1886.325, duration=4.415), FetchedTranscriptSnippet(text='We can now amplify this direction by multiplying by a scaling factor, alpha,', start=1891.68, duration=4.112), FetchedTranscriptSnippet(text='and replace our original conditioned yellow vector with a vector pointing in this new', start=1895.792, duration=4.653), FetchedTranscriptSnippet(text='direction.', start=1900.445, duration=0.595), FetchedTranscriptSnippet(text=\"Let's follow the trajectory of the same cat point we\", start=1901.9, duration=2.517), FetchedTranscriptSnippet(text=\"saw earlier that didn't quite make it onto our spiral.\", start=1904.417, duration=2.663), FetchedTranscriptSnippet(text=\"We'll roll back our diffusion time variable and start\", start=1907.58, duration=2.793), FetchedTranscriptSnippet(text='a new green point from the same starting location.', start=1910.373, duration=2.687), FetchedTranscriptSnippet(text='If we use our new green vectors to guide the diffusion process instead of our original', start=1913.66, duration=4.326), FetchedTranscriptSnippet(text='yellow vectors, the difference between our gray arrows that point towards the center', start=1917.986, duration=4.275), FetchedTranscriptSnippet(text='of our spiral and yellow vectors that start pointing us back towards our cat part', start=1922.261, duration=4.125), FetchedTranscriptSnippet(text='of the spiral are amplified, now guiding our point to land nicely on our spiral.', start=1926.386, duration=4.074), FetchedTranscriptSnippet(text='This approach is called classifier-free guidance.', start=1931.48, duration=2.34), FetchedTranscriptSnippet(text='Using our new green vectors to guide a set of cat points,', start=1935.16, duration=3.163), FetchedTranscriptSnippet(text='we see a nice tight fit to our spiral for this class.', start=1938.323, duration=2.997), FetchedTranscriptSnippet(text='Switching to our dog class, our unconditional gray vector field stays the same,', start=1942.4, duration=4.449), FetchedTranscriptSnippet(text='but our dog conditioned model outputs, shown in magenta,', start=1946.849, duration=3.21), FetchedTranscriptSnippet(text='now point us more towards the dog part of our spiral.', start=1950.059, duration=3.041), FetchedTranscriptSnippet(text='And adding guidance amplifies this learned direction.', start=1953.82, duration=2.72), FetchedTranscriptSnippet(text='Using our guided vectors and running our generation process,', start=1958.28, duration=3.306), FetchedTranscriptSnippet(text='we see a nice fit for our dog points.', start=1961.586, duration=2.094), FetchedTranscriptSnippet(text='Finally, we get a third vector field for our people examples', start=1964.64, duration=2.874), FetchedTranscriptSnippet(text='that again results in nice convergence to our spiral.', start=1967.514, duration=2.586), FetchedTranscriptSnippet(text='Classifier-free guidance works remarkably well and has become an', start=1971.58, duration=3.532), FetchedTranscriptSnippet(text='essential part of many modern image and video generation models.', start=1975.112, duration=3.588), FetchedTranscriptSnippet(text='Earlier, we saw that if we only conditioned our stable diffusion model,', start=1979.52, duration=3.506), FetchedTranscriptSnippet(text='our image would have a desert and a shadow, but no tree that we asked for in the prompt.', start=1983.026, duration=4.394), FetchedTranscriptSnippet(text='If we add classifier-free guidance to this model,', start=1988.48, duration=2.843), FetchedTranscriptSnippet(text='once we reach a guidance scale alpha of around 2,', start=1991.323, duration=2.9), FetchedTranscriptSnippet(text='we start to actually see a tiny tree in our images.', start=1994.223, duration=3.017), FetchedTranscriptSnippet(text='And the size and detail of our tree improve as we increase our scaling factor, alpha.', start=1997.7, duration=4.5), FetchedTranscriptSnippet(text='The fact that this works so well is remarkable to me.', start=2003.38, duration=2.74), FetchedTranscriptSnippet(text=\"As we use guidance to point our stable diffusion model's vector field more in the\", start=2006.68, duration=4.34), FetchedTranscriptSnippet(text='direction of our prompt, our tree literally grows in size and detail in our images.', start=2011.02, duration=4.5), FetchedTranscriptSnippet(text='Our WAN video generation model takes this guidance approach one step further.', start=2017.04, duration=3.98), FetchedTranscriptSnippet(text='Instead of subtracting the output of an unconditioned model with no text input,', start=2021.76, duration=4.125), FetchedTranscriptSnippet(text=\"the WAN team uses what's known as a negative prompt,\", start=2025.885, duration=2.767), FetchedTranscriptSnippet(text=\"where they specifically write out all the features they don't want in their video,\", start=2028.652, duration=4.333), FetchedTranscriptSnippet(text=\"and then subtract the resulting vector from the model's conditioned output\", start=2032.985, duration=3.916), FetchedTranscriptSnippet(text='and amplify the result, steering the diffusion process away from these unwanted features.', start=2036.901, duration=4.699), FetchedTranscriptSnippet(text='Their standard negative prompt is fascinating,', start=2042.82, duration=2.412), FetchedTranscriptSnippet(text='including features like extra fingers and walking backwards,', start=2045.232, duration=3.2), FetchedTranscriptSnippet(text='and interestingly is actually passed into their text encoder in Chinese.', start=2048.432, duration=3.828), FetchedTranscriptSnippet(text=\"Here's a video generated using the same astronaut on a horse prompt we used earlier,\", start=2053.179, duration=4.509), FetchedTranscriptSnippet(text='but without the negative prompt.', start=2057.688, duration=1.772), FetchedTranscriptSnippet(text=\"It's really interesting to see how the parts of the\", start=2059.86, duration=2.437), FetchedTranscriptSnippet(text='scene get cartoonish and no longer fit together.', start=2062.297, duration=2.342), FetchedTranscriptSnippet(text='Since the publication of the DDPM paper in the summer of 2020,', start=2066.179, duration=3.812), FetchedTranscriptSnippet(text='the field has progressed at a blistering pace,', start=2069.991, duration=2.891), FetchedTranscriptSnippet(text='leading to the incredible text-to-video models that we see today.', start=2072.882, duration=4.058), FetchedTranscriptSnippet(text='Of all the interesting details that make these models tick,', start=2078.44, duration=2.964), FetchedTranscriptSnippet(text='the most astounding thing to me is that the pieces fit together at all.', start=2081.404, duration=3.616), FetchedTranscriptSnippet(text='The fact that we can take a trained text encoder from clip or', start=2086.08, duration=3.672), FetchedTranscriptSnippet(text='elsewhere and use its output to actually steer the diffusion process,', start=2089.752, duration=4.215), FetchedTranscriptSnippet(text='which itself is highly complex, seems almost too good to be true.', start=2093.967, duration=3.973), FetchedTranscriptSnippet(text='And on top of that, many of these core ideas can be built from', start=2099.08, duration=3.553), FetchedTranscriptSnippet(text='relatively simple geometric intuitions that somehow hold in', start=2102.633, duration=3.439), FetchedTranscriptSnippet(text='the incredibly high dimensional spaces these models operate in.', start=2106.072, duration=3.668), FetchedTranscriptSnippet(text='The resulting models feel like a fundamentally new class of machine.', start=2110.28, duration=3.9), FetchedTranscriptSnippet(text='To create incredibly lifelike and beautiful images and video,', start=2115.42, duration=3.306), FetchedTranscriptSnippet(text=\"you no longer need a camera, you don't need to know how to draw or how to paint,\", start=2118.726, duration=4.391), FetchedTranscriptSnippet(text='or how to use animation software.', start=2123.117, duration=1.843), FetchedTranscriptSnippet(text='All you need is language.', start=2125.86, duration=1.46), FetchedTranscriptSnippet(text='So this, as you can no doubt tell, was a guest video.', start=2129.56, duration=2.34), FetchedTranscriptSnippet(text='It comes from Stephen Welsh, who runs the channel WelshLabs.', start=2132.24, duration=2.66), FetchedTranscriptSnippet(text=\"If somehow you watch this channel and you're not already familiar with WelshLabs,\", start=2135.28, duration=3.59), FetchedTranscriptSnippet(text=\"you should absolutely go and just watch everything that he's made.\", start=2138.87, duration=2.97), FetchedTranscriptSnippet(text='A while back he made this completely iconic series about imaginary numbers.', start=2142.22, duration=3.44), FetchedTranscriptSnippet(text='He actually has since turned it into a book, and consistent with everything he makes,', start=2146.04, duration=4.185), FetchedTranscriptSnippet(text=\"it's just super high quality, lots of exercises, good stuff like that.\", start=2150.225, duration=3.495), FetchedTranscriptSnippet(text=\"More recently he's been doing a lot of machine learning content,\", start=2154.1, duration=2.713), FetchedTranscriptSnippet(text='so cannot recommend his stuff highly enough.', start=2156.813, duration=1.907), FetchedTranscriptSnippet(text=\"Now the context on why I'm doing guest videos at all is that very\", start=2159.02, duration=2.986), FetchedTranscriptSnippet(text=\"recently my wife and I had our first baby, which I'm very excited about.\", start=2162.006, duration=3.354), FetchedTranscriptSnippet(text=\"And I'm not sure what most solo YouTubers do for paternity leave,\", start=2165.8, duration=3.277), FetchedTranscriptSnippet(text='but the way I decided to go about it was to reach out to a few creators whose work I', start=2169.077, duration=4.285), FetchedTranscriptSnippet(text=\"really enjoy, and who I'm quite sure you're going to enjoy, and essentially ask, hey,\", start=2173.362, duration=4.335), FetchedTranscriptSnippet(text='what do you feel about me pointing some of the Patreon funds that come towards this', start=2177.697, duration=4.235), FetchedTranscriptSnippet(text=\"channel towards you during this time that I'm away,\", start=2181.932, duration=2.621), FetchedTranscriptSnippet(text=\"and kind of commission pieces to fill the airtime while I'm away.\", start=2184.553, duration=3.327), FetchedTranscriptSnippet(text='The pieces are actually going to be really great.', start=2188.7, duration=1.62), FetchedTranscriptSnippet(text=\"I've enjoyed giving some editorial oversight as they're coming in.\", start=2190.44, duration=3.18), FetchedTranscriptSnippet(text=\"You know, we've got statistical mechanics, we've got machine learning,\", start=2193.78, duration=3.135), FetchedTranscriptSnippet(text='even some modern art.', start=2196.915, duration=0.985), FetchedTranscriptSnippet(text=\"It's going to be a good time.\", start=2198.08, duration=0.96), FetchedTranscriptSnippet(text='The next guest video is going to be about a combination of modern art and group theory.', start=2199.18, duration=3.64), FetchedTranscriptSnippet(text=\"It's actually very fun.\", start=2203.12, duration=0.92), FetchedTranscriptSnippet(text=\"And like all the other videos on this channel, if you're a Patreon supporter,\", start=2204.24, duration=3.137), FetchedTranscriptSnippet(text='you can get early views of these ones and provide some feedback before they go live.', start=2207.377, duration=3.463), FetchedTranscriptSnippet(text='Until then, I hope you thoroughly enjoy binge-watching WelshLabs,', start=2211.3, duration=2.798), FetchedTranscriptSnippet(text='and again, consider buying the things that he makes.', start=2214.098, duration=2.282), FetchedTranscriptSnippet(text='There is just as much thought and care put into those as there is into the videos.', start=2216.56, duration=3.14), FetchedTranscriptSnippet(text='Bye!', start=2238.94, duration=0.4)], video_id='iv-5mZ_9CPY', language='English', language_code='en', is_generated=False)\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'FetchedTranscriptSnippet' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m     transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(transcript_list)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     transcript = \u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtranscript_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TranscriptsDisabled:\n\u001b[32m     14\u001b[39m     transcript = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m     transcript_list = YouTubeTranscriptApi().fetch(video_id, languages=[\u001b[33m'\u001b[39m\u001b[33men\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(transcript_list)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     transcript = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m transcript_list)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TranscriptsDisabled:\n\u001b[32m     14\u001b[39m     transcript = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: 'FetchedTranscriptSnippet' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "video_id = \"Gfr50f6ZBvo\" # only the ID, not full URL\n",
        "try:\n",
        "    # If you dont care which language, this returns the best one\n",
        "    transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
        "\n",
        "    # Flatten it to plain text\n",
        "    transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
        "    print(transcript)\n",
        "\n",
        "except TranscriptsDisabled:\n",
        "    print(\"No captions available for this video.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWSK4-VQH8CG",
        "outputId": "01528517-4192-4620-f234-97055e4b6655"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'text': 'Imagine you happen across a short movie script that',\n",
              "  'start': 1.14,\n",
              "  'duration': 2.836},\n",
              " {'text': 'describes a scene between a person and their AI assistant.',\n",
              "  'start': 3.976,\n",
              "  'duration': 3.164},\n",
              " {'text': \"The script has what the person asks the AI, but the AI's response has been torn off.\",\n",
              "  'start': 7.48,\n",
              "  'duration': 5.58},\n",
              " {'text': 'Suppose you also have this powerful magical machine that can take',\n",
              "  'start': 13.06,\n",
              "  'duration': 3.92},\n",
              " {'text': 'any text and provide a sensible prediction of what word comes next.',\n",
              "  'start': 16.98,\n",
              "  'duration': 3.98},\n",
              " {'text': 'You could then finish the script by feeding in what you have to the machine,',\n",
              "  'start': 21.5,\n",
              "  'duration': 4.006},\n",
              " {'text': \"seeing what it would predict to start the AI's answer,\",\n",
              "  'start': 25.506,\n",
              "  'duration': 2.862},\n",
              " {'text': 'and then repeating this over and over with a growing script completing the dialogue.',\n",
              "  'start': 28.368,\n",
              "  'duration': 4.372},\n",
              " {'text': \"When you interact with a chatbot, this is exactly what's happening.\",\n",
              "  'start': 33.38,\n",
              "  'duration': 3.1},\n",
              " {'text': 'A large language model is a sophisticated mathematical function',\n",
              "  'start': 37.02,\n",
              "  'duration': 3.681},\n",
              " {'text': 'that predicts what word comes next for any piece of text.',\n",
              "  'start': 40.701,\n",
              "  'duration': 3.279},\n",
              " {'text': 'Instead of predicting one word with certainty, though,',\n",
              "  'start': 44.38,\n",
              "  'duration': 3.022},\n",
              " {'text': 'what it does is assign a probability to all possible next words.',\n",
              "  'start': 47.402,\n",
              "  'duration': 3.518},\n",
              " {'text': 'To build a chatbot, you lay out some text that describes an interaction between a user',\n",
              "  'start': 51.62,\n",
              "  'duration': 5.18},\n",
              " {'text': 'and a hypothetical AI assistant, add on whatever the user types in as the first part of',\n",
              "  'start': 56.8,\n",
              "  'duration': 5.24},\n",
              " {'text': 'the interaction, and then have the model repeatedly predict the next word that such a',\n",
              "  'start': 62.04,\n",
              "  'duration': 5.12},\n",
              " {'text': \"hypothetical AI assistant would say in response, and that's what's presented to the user.\",\n",
              "  'start': 67.16,\n",
              "  'duration': 5.3},\n",
              " {'text': 'In doing this, the output tends to look a lot more natural if',\n",
              "  'start': 73.08,\n",
              "  'duration': 3.134},\n",
              " {'text': 'you allow it to select less likely words along the way at random.',\n",
              "  'start': 76.214,\n",
              "  'duration': 3.286},\n",
              " {'text': 'So what this means is even though the model itself is deterministic,',\n",
              "  'start': 80.14,\n",
              "  'duration': 3.48},\n",
              " {'text': \"a given prompt typically gives a different answer each time it's run.\",\n",
              "  'start': 83.62,\n",
              "  'duration': 3.48},\n",
              " {'text': 'Models learn how to make these predictions by processing an enormous amount of text,',\n",
              "  'start': 88.04,\n",
              "  'duration': 4.292},\n",
              " {'text': 'typically pulled from the internet.',\n",
              "  'start': 92.332,\n",
              "  'duration': 1.768},\n",
              " {'text': 'For a standard human to read the amount of text that was used to train GPT-3,',\n",
              "  'start': 94.1,\n",
              "  'duration': 5.371},\n",
              " {'text': 'for example, if they read non-stop 24-7, it would take over 2600 years.',\n",
              "  'start': 99.471,\n",
              "  'duration': 4.889},\n",
              " {'text': 'Larger models since then train on much, much more.',\n",
              "  'start': 104.72,\n",
              "  'duration': 2.62},\n",
              " {'text': 'You can think of training a little bit like tuning the dials on a big machine.',\n",
              "  'start': 108.2,\n",
              "  'duration': 3.58},\n",
              " {'text': 'The way that a language model behaves is entirely determined by these',\n",
              "  'start': 112.28,\n",
              "  'duration': 4.021},\n",
              " {'text': 'many different continuous values, usually called parameters or weights.',\n",
              "  'start': 116.301,\n",
              "  'duration': 4.079},\n",
              " {'text': 'Changing those parameters will change the probabilities',\n",
              "  'start': 121.02,\n",
              "  'duration': 3.079},\n",
              " {'text': 'that the model gives for the next word on a given input.',\n",
              "  'start': 124.099,\n",
              "  'duration': 3.081},\n",
              " {'text': 'What puts the large in large language model is how',\n",
              "  'start': 127.86,\n",
              "  'duration': 2.867},\n",
              " {'text': 'they can have hundreds of billions of these parameters.',\n",
              "  'start': 130.727,\n",
              "  'duration': 3.093},\n",
              " {'text': 'No human ever deliberately sets those parameters.',\n",
              "  'start': 135.2,\n",
              "  'duration': 2.84},\n",
              " {'text': 'Instead, they begin at random, meaning the model just outputs gibberish,',\n",
              "  'start': 138.44,\n",
              "  'duration': 4.203},\n",
              " {'text': \"but they're repeatedly refined based on many example pieces of text.\",\n",
              "  'start': 142.643,\n",
              "  'duration': 3.917},\n",
              " {'text': 'One of these training examples could be just a handful of words,',\n",
              "  'start': 147.14,\n",
              "  'duration': 3.516},\n",
              " {'text': 'or it could be thousands, but in either case, the way this works is to',\n",
              "  'start': 150.656,\n",
              "  'duration': 3.84},\n",
              " {'text': 'pass in all but the last word from that example into the model and',\n",
              "  'start': 154.496,\n",
              "  'duration': 3.624},\n",
              " {'text': 'compare the prediction that it makes with the true last word from the example.',\n",
              "  'start': 158.12,\n",
              "  'duration': 4.22},\n",
              " {'text': 'An algorithm called backpropagation is used to tweak all of the parameters',\n",
              "  'start': 163.26,\n",
              "  'duration': 4.133},\n",
              " {'text': 'in such a way that it makes the model a little more likely to choose',\n",
              "  'start': 167.393,\n",
              "  'duration': 3.803},\n",
              " {'text': 'the true last word and a little less likely to choose all the others.',\n",
              "  'start': 171.196,\n",
              "  'duration': 3.804},\n",
              " {'text': 'When you do this for many, many trillions of examples,',\n",
              "  'start': 175.74,\n",
              "  'duration': 3.01},\n",
              " {'text': 'not only does the model start to give more accurate predictions on the training data,',\n",
              "  'start': 178.75,\n",
              "  'duration': 4.708},\n",
              " {'text': \"but it also starts to make more reasonable predictions on text that it's never\",\n",
              "  'start': 183.458,\n",
              "  'duration': 4.325},\n",
              " {'text': 'seen before.', 'start': 187.783, 'duration': 0.657},\n",
              " {'text': 'Given the huge number of parameters and the enormous amount of training data,',\n",
              "  'start': 189.42,\n",
              "  'duration': 4.499},\n",
              " {'text': 'the scale of computation involved in training a large language model is mind-boggling.',\n",
              "  'start': 193.919,\n",
              "  'duration': 4.961},\n",
              " {'text': 'To illustrate, imagine that you could perform one',\n",
              "  'start': 199.6,\n",
              "  'duration': 2.685},\n",
              " {'text': 'billion additions and multiplications every single second.',\n",
              "  'start': 202.285,\n",
              "  'duration': 3.115},\n",
              " {'text': 'How long do you think it would take for you to do all of the',\n",
              "  'start': 206.06,\n",
              "  'duration': 3.266},\n",
              " {'text': 'operations involved in training the largest language models?',\n",
              "  'start': 209.326,\n",
              "  'duration': 3.214},\n",
              " {'text': 'Do you think it would take a year?',\n",
              "  'start': 213.46,\n",
              "  'duration': 1.579},\n",
              " {'text': 'Maybe something like 10,000 years?',\n",
              "  'start': 216.039,\n",
              "  'duration': 1.921},\n",
              " {'text': 'The answer is actually much more than that.',\n",
              "  'start': 219.02,\n",
              "  'duration': 1.78},\n",
              " {'text': \"It's well over 100 million years.\",\n",
              "  'start': 221.12,\n",
              "  'duration': 2.78},\n",
              " {'text': 'This is only part of the story, though.',\n",
              "  'start': 225.52,\n",
              "  'duration': 1.84},\n",
              " {'text': 'This whole process is called pre-training.',\n",
              "  'start': 227.54,\n",
              "  'duration': 1.68},\n",
              " {'text': 'The goal of auto-completing a random passage of text from the',\n",
              "  'start': 229.5,\n",
              "  'duration': 3.146},\n",
              " {'text': 'internet is very different from the goal of being a good AI assistant.',\n",
              "  'start': 232.646,\n",
              "  'duration': 3.554},\n",
              " {'text': 'To address this, chatbots undergo another type of training,',\n",
              "  'start': 236.88,\n",
              "  'duration': 3.2},\n",
              " {'text': 'just as important, called reinforcement learning with human feedback.',\n",
              "  'start': 240.08,\n",
              "  'duration': 3.68},\n",
              " {'text': 'Workers flag unhelpful or problematic predictions,',\n",
              "  'start': 244.48,\n",
              "  'duration': 3.018},\n",
              " {'text': \"and their corrections further change the model's parameters,\",\n",
              "  'start': 247.498,\n",
              "  'duration': 3.611},\n",
              " {'text': 'making them more likely to give predictions that users prefer.',\n",
              "  'start': 251.109,\n",
              "  'duration': 3.671},\n",
              " {'text': 'Looking back at the pre-training, though, this staggering amount of',\n",
              "  'start': 254.78,\n",
              "  'duration': 4.08},\n",
              " {'text': 'computation is only made possible by using special computer chips that',\n",
              "  'start': 258.86,\n",
              "  'duration': 4.26},\n",
              " {'text': 'are optimized for running many operations in parallel, known as GPUs.',\n",
              "  'start': 263.12,\n",
              "  'duration': 4.14},\n",
              " {'text': 'However, not all language models can be easily parallelized.',\n",
              "  'start': 268.12,\n",
              "  'duration': 3.5},\n",
              " {'text': 'Prior to 2017, most language models would process text one word at a time,',\n",
              "  'start': 272.08,\n",
              "  'duration': 4.737},\n",
              " {'text': 'but then a team of researchers at Google introduced a new model known as the transformer.',\n",
              "  'start': 276.817,\n",
              "  'duration': 5.623},\n",
              " {'text': \"Transformers don't read text from the start to the finish,\",\n",
              "  'start': 283.3,\n",
              "  'duration': 3.445},\n",
              " {'text': 'they soak it all in at once, in parallel.',\n",
              "  'start': 286.745,\n",
              "  'duration': 2.395},\n",
              " {'text': 'The very first step inside a transformer, and most other language models for that matter,',\n",
              "  'start': 289.9,\n",
              "  'duration': 4.7},\n",
              " {'text': 'is to associate each word with a long list of numbers.',\n",
              "  'start': 294.6,\n",
              "  'duration': 2.82},\n",
              " {'text': 'The reason for this is that the training process only works with continuous values,',\n",
              "  'start': 297.86,\n",
              "  'duration': 4.536},\n",
              " {'text': 'so you have to somehow encode language using numbers,',\n",
              "  'start': 302.396,\n",
              "  'duration': 2.916},\n",
              " {'text': 'and each of these lists of numbers may somehow encode the meaning of the',\n",
              "  'start': 305.312,\n",
              "  'duration': 3.942},\n",
              " {'text': 'corresponding word.', 'start': 309.254, 'duration': 1.026},\n",
              " {'text': 'What makes transformers unique is their reliance',\n",
              "  'start': 310.28,\n",
              "  'duration': 3.08},\n",
              " {'text': 'on a special operation known as attention.',\n",
              "  'start': 313.36,\n",
              "  'duration': 2.64},\n",
              " {'text': 'This operation gives all of these lists of numbers a chance to talk to one another',\n",
              "  'start': 316.98,\n",
              "  'duration': 4.704},\n",
              " {'text': 'and refine the meanings they encode based on the context around, all done in parallel.',\n",
              "  'start': 321.684,\n",
              "  'duration': 4.876},\n",
              " {'text': 'For example, the numbers encoding the word bank might be changed based on the',\n",
              "  'start': 327.4,\n",
              "  'duration': 4.307},\n",
              " {'text': 'context surrounding it to somehow encode the more specific notion of a riverbank.',\n",
              "  'start': 331.707,\n",
              "  'duration': 4.473},\n",
              " {'text': 'Transformers typically also include a second type of operation known',\n",
              "  'start': 337.28,\n",
              "  'duration': 3.749},\n",
              " {'text': 'as a feed-forward neural network, and this gives the model extra',\n",
              "  'start': 341.029,\n",
              "  'duration': 3.532},\n",
              " {'text': 'capacity to store more patterns about language learned during training.',\n",
              "  'start': 344.561,\n",
              "  'duration': 3.859},\n",
              " {'text': 'All of this data repeatedly flows through many different iterations of',\n",
              "  'start': 349.28,\n",
              "  'duration': 4.121},\n",
              " {'text': 'these two fundamental operations, and as it does so,',\n",
              "  'start': 353.401,\n",
              "  'duration': 3.077},\n",
              " {'text': 'the hope is that each list of numbers is enriched to encode whatever',\n",
              "  'start': 356.478,\n",
              "  'duration': 4.006},\n",
              " {'text': 'information might be needed to make an accurate prediction of what word',\n",
              "  'start': 360.484,\n",
              "  'duration': 4.18},\n",
              " {'text': 'follows in the passage.', 'start': 364.664, 'duration': 1.336},\n",
              " {'text': 'At the end, one final function is performed on the last vector in this sequence,',\n",
              "  'start': 367.0,\n",
              "  'duration': 4.534},\n",
              " {'text': 'which now has had a chance to be influenced by all the other context from the input text,',\n",
              "  'start': 371.534,\n",
              "  'duration': 5.039},\n",
              " {'text': 'as well as everything the model learned during training,',\n",
              "  'start': 376.573,\n",
              "  'duration': 3.191},\n",
              " {'text': 'to produce a prediction of the next word.',\n",
              "  'start': 379.764,\n",
              "  'duration': 2.296},\n",
              " {'text': \"Again, the model's prediction looks like a probability for every possible next word.\",\n",
              "  'start': 382.48,\n",
              "  'duration': 4.88},\n",
              " {'text': 'Although researchers design the framework for how each of these steps work,',\n",
              "  'start': 388.56,\n",
              "  'duration': 4.234},\n",
              " {'text': \"it's important to understand that the specific behavior is an emergent phenomenon\",\n",
              "  'start': 392.794,\n",
              "  'duration': 4.568},\n",
              " {'text': 'based on how those hundreds of billions of parameters are tuned during training.',\n",
              "  'start': 397.362,\n",
              "  'duration': 4.458},\n",
              " {'text': 'This makes it incredibly challenging to determine',\n",
              "  'start': 402.48,\n",
              "  'duration': 2.59},\n",
              " {'text': 'why the model makes the exact predictions that it does.',\n",
              "  'start': 405.07,\n",
              "  'duration': 2.85},\n",
              " {'text': 'What you can see is that when you use large language model predictions to autocomplete',\n",
              "  'start': 408.44,\n",
              "  'duration': 5.338},\n",
              " {'text': 'a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.',\n",
              "  'start': 413.778,\n",
              "  'duration': 5.462},\n",
              " {'text': \"If you're a new viewer and you're curious about more details on how\",\n",
              "  'start': 425.719,\n",
              "  'duration': 3.107},\n",
              " {'text': 'transformers and attention work, boy do I have some material for you.',\n",
              "  'start': 428.826,\n",
              "  'duration': 3.153},\n",
              " {'text': 'One option is to jump into a series I made about deep learning,',\n",
              "  'start': 432.399,\n",
              "  'duration': 3.681},\n",
              " {'text': 'where we visualize and motivate the details of attention and all the other steps',\n",
              "  'start': 436.08,\n",
              "  'duration': 4.66},\n",
              " {'text': 'in a transformer.', 'start': 440.74, 'duration': 0.979},\n",
              " {'text': 'Also, on my second channel I just posted a talk I gave a couple',\n",
              "  'start': 442.099,\n",
              "  'duration': 3.43},\n",
              " {'text': 'months ago about this topic for the company TNG in Munich.',\n",
              "  'start': 445.529,\n",
              "  'duration': 3.11},\n",
              " {'text': 'Sometimes I actually prefer the content I make as a casual talk rather than a produced',\n",
              "  'start': 449.079,\n",
              "  'duration': 4.022},\n",
              " {'text': 'video, but I leave it up to you which one of these feels like the better follow-on.',\n",
              "  'start': 453.101,\n",
              "  'duration': 3.838}]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcript_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKkcYsaOCrRX"
      },
      "source": [
        "## Step 1b - Indexing (Text Splitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24i-ZSVXFbnC"
      },
      "outputs": [],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.create_documents([transcript])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dm9sfpQFnF1",
        "outputId": "7b9bea3d-b5a4-47f1-f793-16b6bfdd6a7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "168"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYlrcBrkFO-N",
        "outputId": "b6e33df4-c8a2-4c2d-f929-4af59774bd94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={}, page_content=\"and and kind of come up with descriptions of the electron clouds where they're gonna go how they're gonna interact when you put two elements together uh and what we try to do is learn a simulation uh uh learner functional that will describe more chemistry types of chemistry so um until now you know you can run expensive simulations but then you can only simulate very small uh molecules very simple molecules we would like to simulate large materials um and so uh today there's no way of doing that and we're building up towards uh building functionals that approximate schrodinger's equation and then allow you to describe uh what the electrons are doing and all materials sort of science and material properties are governed by the electrons and and how they interact so have a good summarization of the simulation through the functional um but one that is still close to what the actual simulation would come out with so what um how difficult is that to ask what's involved in that task is it\")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xYFK7WXC2Ka"
      },
      "source": [
        "## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYXeS5T7FrC4"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWYkp-NmFSVF",
        "outputId": "36f75b4d-b798-4e06-aeea-5c56c91befe0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '7c0f7504-08fa-4f33-8708-d29bfc601f84',\n",
              " 1: '06b75197-142c-4ac0-88c8-1bf9e865763b',\n",
              " 2: '6ff986db-4118-4212-a5fe-39f0cf922677',\n",
              " 3: '447cb65c-cfa2-43aa-ad48-2321366e5762',\n",
              " 4: '30f35db0-6d97-4a6f-b246-27c4cbcb6d13',\n",
              " 5: '3fd10381-afd7-427e-958e-89fc520dc260',\n",
              " 6: '40da6068-0065-4411-9417-11f839de969c',\n",
              " 7: '733cdcc4-7685-40a2-9164-413b8fc4366d',\n",
              " 8: '03adb4d2-e52b-4efd-911d-f7ae4caa4eaa',\n",
              " 9: 'e6997d55-f5b1-4785-bc63-613d57306c68',\n",
              " 10: 'ec55a375-5d7c-46b5-9d0c-73e8f8810712',\n",
              " 11: '2b8a87c8-01a6-4390-a751-543668f921d9',\n",
              " 12: '4a0fa4f7-0abd-4e05-9154-2d90d7f42c9e',\n",
              " 13: 'f36acaa7-947f-4120-8094-5a0a3521ad9a',\n",
              " 14: '87dc4c33-6bcd-4b55-b2ca-1e1d2e30e374',\n",
              " 15: 'bfbfd51c-e879-4c43-8d0f-97cdea1ecf7d',\n",
              " 16: '237bef6d-bc86-49b6-86a3-f300c6cd96de',\n",
              " 17: '62de1f42-4306-40c4-aa52-ab93a2d43754',\n",
              " 18: '22ac2726-8b22-48b7-a457-c9b81799f2a7',\n",
              " 19: '92d2f60c-80d5-4c12-ad2b-cb30a98851e8',\n",
              " 20: '23b0db0d-da2e-4ab9-84db-0ae8c5d15a42',\n",
              " 21: '08bc0031-2f11-468c-a1dd-54449e12db06',\n",
              " 22: 'eb3b8f90-9ee9-44a8-8aee-c5cb784c7eb3',\n",
              " 23: 'c7f6986a-189a-4444-a900-e1e568caee9c',\n",
              " 24: 'e3d7ab3b-7b60-4de0-8a1e-3b8394003fee',\n",
              " 25: '823ab093-dd84-40ba-9c4e-ddab818085b8',\n",
              " 26: 'b4fb574d-9bc3-4c94-9df0-5212275fbb2d',\n",
              " 27: '812a72a5-fdbd-4867-a877-18e807a8d774',\n",
              " 28: '2bcda52b-0e25-4798-bf2a-c8229d70ec57',\n",
              " 29: '4cd08b34-05ce-4f79-b0da-667f71f44936',\n",
              " 30: '57ab4815-bd42-4664-8976-dd4fc53e770d',\n",
              " 31: 'fff29621-f307-44ea-8e14-34d276c124cd',\n",
              " 32: '656a4467-530b-4137-9e60-5a409aa62345',\n",
              " 33: '7da6c567-238a-400b-8ce7-91ee4ef9ccd3',\n",
              " 34: '8234c825-74a1-45ee-a683-b264eb11c29b',\n",
              " 35: '0d860b3c-c53c-451a-b761-b782785cbab4',\n",
              " 36: '19fdaa54-8c4d-4fd4-8b1c-55ab1f3b1fc1',\n",
              " 37: 'd720c8cf-c224-4c15-b494-116bd5d30424',\n",
              " 38: '56ede6f0-d62d-4a42-9b13-d847cf46532e',\n",
              " 39: 'acf86bb1-83b1-4efc-89ef-b0530fc50c18',\n",
              " 40: 'e0ba422e-a794-4ef2-aa51-a75d66e9d9db',\n",
              " 41: '5f46e4dd-022c-40d8-b73c-0901e428b13c',\n",
              " 42: '374e910b-041c-4006-b94b-1a37194ad549',\n",
              " 43: '6b5bc2fc-eb43-428e-8ecb-7874352e1b42',\n",
              " 44: '1add167b-2504-4d51-8974-d0b7644f8c76',\n",
              " 45: '163d291f-437c-4280-af18-7484737e1648',\n",
              " 46: '45bf20b3-7e51-4978-8efa-ad7e4a4b866a',\n",
              " 47: 'ee411984-e0bc-443f-992e-bc5cf3111f3d',\n",
              " 48: '18b51f36-3fff-4b7f-a507-1c9ce1a8982d',\n",
              " 49: 'e0eb5bd3-a9bb-4ceb-97df-85c9dabb5a53',\n",
              " 50: '4c55b502-fce1-4360-8076-42a5db26107a',\n",
              " 51: '5c768812-1807-4678-bdd8-81d1582e2e8c',\n",
              " 52: 'fbd6fcc0-7b6f-4d3c-af02-b2e5ffccbd37',\n",
              " 53: '23807eb3-ed23-4029-bd8d-0db40db80c5d',\n",
              " 54: '9bacf012-cc41-4136-82ed-29a5d4a539a4',\n",
              " 55: 'a3c14979-7346-4d9f-bc39-b8194e4582de',\n",
              " 56: 'ce310fcb-b6f2-4f03-8054-eeafdf2a2bf1',\n",
              " 57: 'a9d46f8a-f7da-4bd0-898c-a13e5439b556',\n",
              " 58: 'fb5e7e98-0bd0-4049-ad0e-a38fb835c2b9',\n",
              " 59: '6fc1a125-73f1-4560-8b0a-e5ba6fd9d403',\n",
              " 60: 'cf11b0a4-d350-4368-bc85-bcf92e275e5c',\n",
              " 61: '3c57fe3f-4c89-48b6-979f-fbca4afdd1d3',\n",
              " 62: '3a6e36df-cf23-4ffe-9d12-49c6b05dc5a2',\n",
              " 63: '07799ca1-a873-4a15-8886-b6fb19d93421',\n",
              " 64: '4b68456d-1e53-4e86-86b2-a8a131a11cb0',\n",
              " 65: 'a29fe24c-d162-41b8-b057-dcf0dc7b987b',\n",
              " 66: '066d9da7-cf6d-4a0c-af6e-5457d9a0675f',\n",
              " 67: '431072f6-d213-4757-98f9-5e4b76b4bec4',\n",
              " 68: '5404b92d-be79-44b1-bf76-bb347bb9dac9',\n",
              " 69: 'ddff1076-f241-45ca-bab9-3e03b1a812e5',\n",
              " 70: '934adbd6-6408-45da-abc5-ebba1785562b',\n",
              " 71: '0ddf7e65-906a-4871-8a0d-f277750dc64c',\n",
              " 72: 'd1e52fd8-c3ae-429b-beab-bcd2e4d26e8b',\n",
              " 73: '20059f0a-9666-45c0-a7b3-2fda7d3b490e',\n",
              " 74: '88610b3f-60af-46c4-9d5b-b76ad086c195',\n",
              " 75: '72b7958a-f509-4348-b49d-ce3ce436afba',\n",
              " 76: 'a3d1ff48-f964-4a66-8b67-d798004f9b54',\n",
              " 77: '24535063-4e37-4196-b8a7-1bdde5382cc6',\n",
              " 78: '798e39ba-4075-4279-904a-d1fc5391bdb1',\n",
              " 79: '81c22ad4-0553-471f-8f02-e4c99c595ec9',\n",
              " 80: '4c2bd757-2bbb-4c72-93f6-b3ea60eb4f91',\n",
              " 81: '46758929-139f-400f-adef-9610f24674a5',\n",
              " 82: '5de22cf4-16d2-4081-a481-709ec357853d',\n",
              " 83: '55806ba1-fbcb-4990-8624-741288e481c7',\n",
              " 84: 'b7e9e8b6-62e5-4ff4-98e0-f64405835a8e',\n",
              " 85: '6f5a3e09-505d-4d93-8385-3b5b5b5934a0',\n",
              " 86: '927c1070-b0e5-412a-acc7-e0228579cfcc',\n",
              " 87: 'dea4fd63-732b-4f24-9d4a-9ce4e91fa641',\n",
              " 88: '99d8bfff-70c8-447d-bbb4-aa02cfbf29f9',\n",
              " 89: '3eb4d7b9-7e8e-47b6-aff4-be9e272b1937',\n",
              " 90: 'b3d00b0f-a8ef-4dc6-aae5-a538ca3355af',\n",
              " 91: 'ba549b8b-25db-4854-b8b8-41952da6fddb',\n",
              " 92: '14f4851b-7f23-46fc-a961-f82640283e5d',\n",
              " 93: 'b2ae8f4e-d4bc-4c3c-a7e0-0f13fbfc2919',\n",
              " 94: 'a1dc5bd4-b901-4d17-b77e-f573d2d20f86',\n",
              " 95: 'fd83b803-2da9-433d-ac21-8961e541214e',\n",
              " 96: '17202855-855f-4792-86c9-c16f8d99648d',\n",
              " 97: '66794e6a-73e9-4d19-b28e-adafa60ae70b',\n",
              " 98: '3c60d0d6-5d01-4dfc-99fc-5c4bb4422cb0',\n",
              " 99: '46a885b0-6a22-49a7-88a2-1582b71eca30',\n",
              " 100: '27b32505-e6ef-4995-9133-9f0eea4f7756',\n",
              " 101: 'e18f33d7-e7a7-4ddb-8591-4e5c7c1d372c',\n",
              " 102: 'a2d388ed-5b21-42b8-a1d2-a1fdf505b9c6',\n",
              " 103: 'a847078b-7f8f-4132-b035-f7c640309c99',\n",
              " 104: '2585c5b6-31ae-4f25-b934-5e7cc42f68bd',\n",
              " 105: 'c0c4a613-b5df-4e18-bb09-a4efd45d5184',\n",
              " 106: '22bf50e1-0946-4ef8-8d5a-0b704d1b7ed6',\n",
              " 107: '90f7e684-e591-4a13-b3bc-5ac1abfc57df',\n",
              " 108: 'e23879ad-4d5d-4418-a8c4-ee57064ab884',\n",
              " 109: '8eddc30e-ff7a-4419-9e07-d055eb056f73',\n",
              " 110: '93c2f66e-1dbc-4e8a-922a-168e8b952852',\n",
              " 111: '61f745b3-6b56-4e5f-8db0-18a8fb7846ee',\n",
              " 112: 'e862f216-444b-4ef3-ae50-2c8bc2480998',\n",
              " 113: '20d73cc0-c41a-4cfe-8235-04f1aad627e5',\n",
              " 114: 'f2f48f74-9482-4128-8dba-d44a057bd2c0',\n",
              " 115: '87dac438-291e-4aa9-ad65-d2dbf3f01297',\n",
              " 116: 'f4b1a30f-ae12-4dcc-9874-9ea203740af2',\n",
              " 117: '0d6e549f-50fe-4d15-bd37-f9475fbafc65',\n",
              " 118: '3f7e1cc1-c2f4-4535-ac49-959571c91f20',\n",
              " 119: '49aabcb4-b26f-49f4-84df-c9837998d954',\n",
              " 120: 'a2371319-6f39-470e-adaf-104ecbe3d7f1',\n",
              " 121: '03f1d880-48f1-43d6-b776-0f15d56f99fa',\n",
              " 122: 'bf7b60c6-bbc6-4d80-a32a-f996025b075b',\n",
              " 123: '5bf9a83d-d4d2-4dd6-ba80-0956d8847b0a',\n",
              " 124: '22748507-a220-4065-b569-5a4fe90e7367',\n",
              " 125: 'c08ae5d4-16c0-4099-8d46-e88899769460',\n",
              " 126: '957cc44d-376f-4f5b-8b57-f2cca92d355c',\n",
              " 127: '7ed9015a-7e17-4c46-bf65-d4560d7031d1',\n",
              " 128: '6b0cd719-8f14-42a0-88fe-467b7c6ba694',\n",
              " 129: '4d4d6af4-0023-42d3-9c1e-0663e32847dc',\n",
              " 130: '1fe7ff26-91f1-4e4c-94df-4a3aa4d3cfce',\n",
              " 131: 'cba11826-b8ae-4258-87e8-6ad9f3761680',\n",
              " 132: '2f2617cd-53b0-4619-ba58-d09300a8889c',\n",
              " 133: '72bc0f75-acc7-4e51-9bd9-2bf7dc51ac73',\n",
              " 134: 'fd0ddd22-a60b-455c-ab78-17775c2dc841',\n",
              " 135: 'aea872f1-ec68-4b11-b091-a604292dbe23',\n",
              " 136: 'c49b75f3-3978-4692-9c44-f3e8a3800bfc',\n",
              " 137: '40aaa374-3d8e-4ef5-9277-1aa673740db7',\n",
              " 138: '92979d26-044c-4454-baaa-ff4ad016703d',\n",
              " 139: 'aa7d5183-fbc3-4112-8fa7-5379f20eb288',\n",
              " 140: 'a714c2eb-4318-444a-a1ed-0096bb018e6a',\n",
              " 141: '59a18bbe-b7c4-4fe9-a1e3-bd123b0e3294',\n",
              " 142: '8c458c0d-113e-4816-99a2-1e31ff26804d',\n",
              " 143: '8e0c045f-4db0-46d7-a95f-daaad108bed0',\n",
              " 144: '3dd998c8-c503-4f44-af5c-9123b6eded0e',\n",
              " 145: '591d8d55-8d0c-4176-a8b0-d12cca88c515',\n",
              " 146: '3f2c5c9e-2fc8-4d7a-8516-9c541b19f58c',\n",
              " 147: '727aed50-04c4-4204-b778-182b15014c8a',\n",
              " 148: '8d688ab9-5c84-4aaa-bc9a-70def267332e',\n",
              " 149: 'e0aee3bf-32a2-4d82-a6b5-687fcb89b897',\n",
              " 150: 'a689d18d-de55-420c-91e1-1720142606c9',\n",
              " 151: 'a6a3fbc6-2087-41df-9853-e6a2ce7d8318',\n",
              " 152: '773efd96-3129-495b-b8ce-453744e2e6bf',\n",
              " 153: '81a018fe-f692-4cdf-ab6b-593961becd3f',\n",
              " 154: '5b681d5e-9e38-4ce4-9fb3-c23fdcedce48',\n",
              " 155: '80a35423-cc8a-4a56-a5a7-fc211c758fa6',\n",
              " 156: 'ab1f6a95-ff8c-4aa8-9046-8e4de91a84c7',\n",
              " 157: 'c0e69797-0677-44fb-9576-5c624b6036fc',\n",
              " 158: '7d14b9ff-7ab2-42eb-b259-aaa839f74181',\n",
              " 159: '306f52e0-b933-458a-b4e5-3e953f4eb30b',\n",
              " 160: '8f8614f3-4d90-4244-8728-79e1eb02e86e',\n",
              " 161: '8452e7ca-6211-4d27-9b21-c23d17396311',\n",
              " 162: '8420569e-0351-488d-af65-1f8445315f2c',\n",
              " 163: '5a65ec2b-b76f-43cb-9a75-48684ece1561',\n",
              " 164: '66d04fbf-e4b5-4d0f-ae94-f40d07c4ca66',\n",
              " 165: '31dca5d1-666b-4aab-af3d-540d1c4f093d',\n",
              " 166: 'ea5a8f69-dd04-408e-8351-d00b299cda65',\n",
              " 167: '2436bdb8-3f5f-49c6-8915-0c654c888700'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.index_to_docstore_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxokTcWEGGAo",
        "outputId": "e05f7a2a-4cb1-4fb0-ba48-e6887877a260"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='2436bdb8-3f5f-49c6-8915-0c654c888700', metadata={}, page_content='demas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than astronomy is about telescopes thank you for listening and hope to see you next time')]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.get_by_ids(['2436bdb8-3f5f-49c6-8915-0c654c888700'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zez1650EDN9J"
      },
      "source": [
        "## Step 2 - Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEuoGUYOF3oG"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcose8VuGFAv",
        "outputId": "f8e5472c-4073-4e44-af3a-f4ffcd023dc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fdba029d2d0>, search_kwargs={'k': 4})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvrsq08TGGNk",
        "outputId": "55ed9475-4497-4e53-d380-5c4c10bf68cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='7c0f7504-08fa-4f33-8708-d29bfc601f84', metadata={}, page_content=\"the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible artificial intelligence systems in the history of computing including alfred zero that learned all by itself to play the game of gold better than any human in the world and alpha fold two that solved protein folding both tasks considered nearly impossible for a very long time demus is widely considered to be one of the most brilliant and impactful humans in the history of artificial intelligence and science and engineering in general this was truly an honor and a pleasure for me to finally sit down with him for this conversation and i'm sure we will talk many times again in the future this is the lex friedman podcast to support it please check out our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough\"),\n",
              " Document(id='431072f6-d213-4757-98f9-5e4b76b4bec4', metadata={}, page_content='i used to discuss um uh uh what were the sort of founding tenets of deep mind and it was very various things one was um algorithmic advances so deep learning you know jeff hinton and cohen just had just sort of invented that in academia but no one in industry knew about it uh we love reinforcement learning we thought that could be scaled up but also understanding about the human brain had advanced um quite a lot uh in the decade prior with fmri machines and other things so we could get some good hints about architectures and algorithms and and sort of um representations maybe that the brain uses so as at a systems level not at a implementation level um and then the other big things were compute and gpus right so we could see a compute was going to be really useful and it got to a place where it became commoditized mostly through the games industry and and that could be taken advantage of and then the final thing was also mathematical and theoretical definitions of intelligence so'),\n",
              " Document(id='4b68456d-1e53-4e86-86b2-a8a131a11cb0', metadata={}, page_content=\"and how it works this is tough to uh ask you this question because you probably will say it's everything but let's let's try let's try to think to this because you're in a very interesting position where deepmind is the place of some of the most uh brilliant ideas in the history of ai but it's also a place of brilliant engineering so how much of solving intelligence this big goal for deepmind how much of it is science how much is engineering so how much is the algorithms how much is the data how much is the hardware compute infrastructure how much is it the software computer infrastructure yeah um what else is there how much is the human infrastructure and like just the humans interact in certain kinds of ways in all the space of all those ideas how much does maybe like philosophy how much what's the key if um uh if if you were to sort of look back like if we go forward 200 years look back what was the key thing that solved intelligence is that ideas i think it's a combination first\"),\n",
              " Document(id='20059f0a-9666-45c0-a7b3-2fda7d3b490e', metadata={}, page_content=\"ambitious as trying to solve intelligence and you're you're you know it's blue sky research no one knows how to do it you you you need to use any evidence or any source of information you can to help guide you in the right direction or give you confidence you're going in the right direction so so that that was one reason we pushed so hard on that and that's and just going back to your early question about organization the other big thing that i think we innovated with at deepmind to encourage invention and and uh and innovation was the multi-disciplinary organization we built and we still have today so deepmind originally was a confluence of the of the most cutting-edge knowledge in neuroscience with machine learning engineering and mathematics right and and gaming and then since then we built that out even further so we have philosophers here and and uh by you know ethicists but also other types of scientists physicists and so on um and that's what brings together i tried to build a\")]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever.invoke('What is deepmind')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8y0wRmoDSVZ"
      },
      "source": [
        "## Step 3 - Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2P2AlJ0GN5L"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-NeLx9wFHzw"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "      You are a helpful assistant.\n",
        "      Answer ONLY from the provided transcript context.\n",
        "      If the context is insufficient, just say you don't know.\n",
        "\n",
        "      {context}\n",
        "      Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = ['context', 'question']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI9BOZQwGizf"
      },
      "outputs": [],
      "source": [
        "question          = \"is the topic of nuclear fusion discussed in this video? if yes then what was discussed\"\n",
        "retrieved_docs    = retriever.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfv8yNFsK_GN",
        "outputId": "79fa2a7e-8d92-45bf-99e4-ad12cd86c7a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='3c60d0d6-5d01-4dfc-99fc-5c4bb4422cb0', metadata={}, page_content=\"so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\"),\n",
              " Document(id='fd83b803-2da9-433d-ac21-8961e541214e', metadata={}, page_content=\"in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\"),\n",
              " Document(id='a1dc5bd4-b901-4d17-b77e-f573d2d20f86', metadata={}, page_content='that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double'),\n",
              " Document(id='b2ae8f4e-d4bc-4c3c-a7e0-0f13fbfc2919', metadata={}, page_content=\"like room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\")]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieved_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "bKwpvAo5G_Pk",
        "outputId": "26f0efd0-b35f-44dd-9735-dfc41c1ab86d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\\n\\nin this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\\n\\nthat i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double\\n\\nlike room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\""
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bikWKZWDiqB"
      },
      "outputs": [],
      "source": [
        "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOFVVAbLYvU",
        "outputId": "b6c47460-fdf3-4dad-ebe5-611b3dfe854b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n      You are a helpful assistant.\\n      Answer ONLY from the provided transcript context.\\n      If the context is insufficient, just say you don't know.\\n\\n      so we with this problem and we published it in a nature paper last year uh we held the fusion that we held the plasma in specific shapes so actually it's almost like carving the plasma into different shapes and control and hold it there for the record amount of time so um so that's one of the problems of of fusion sort of um solved so i have a controller that's able to no matter the shape uh contain it continue yeah contain it and hold it in structure and there's different shapes that are better for for the energy productions called droplets and and and so on so um so that was huge and now we're looking we're talking to lots of fusion startups to see what's the next problem we can tackle uh in the fusion area so another fascinating place in a paper title pushing the frontiers of density functionals by solving the fractional electron problem so you're taking on modeling and simulating the quantum mechanical behavior of electrons yes um can you explain this work and can ai model and\\n\\nin this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double checked with the team we were going to use carefully and safely i was impressed they managed to persuade them to let us use it and um and it's a it's an amazing test reactor they have there and they try all sorts of pretty crazy experiments on it and um the the the what we tend to look at is if we go into a new domain like fusion what are all the bottleneck problems uh like thinking from first principles you know what are all the bottleneck problems that are still stopping fusion working today and then we look at we you know we get a fusion expert to tell us and then we look at those bottlenecks and we look at the ones which ones are amenable to our ai methods today yes right and and and then and would be interesting from a research perspective from our point of view from an ai point of\\n\\nthat i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so energy and climate so we talked about disease and biology as being one of the biggest places i think ai can help with i think energy and climate uh is another one so maybe they would be my top two um and fusion is one one area i think ai can help with now fusion has many challenges mostly physics material science and engineering challenges as well to build these massive fusion reactors and contain the plasma and what we try to do whenever we go into a new field to apply our systems is we look for um we talk to domain experts we try and find the best people in the world to collaborate with um in this case in fusion we we collaborated with epfl in switzerland the swiss technical institute who are amazing they have a test reactor that they were willing to let us use which you know i double\\n\\nlike room temperature superconductors or something on my list one day i'd like to like you know have an ai system to help build better optimized batteries all of these sort of mechanical things mr i think a systematic sort of search could be uh guided by a model could be um could be extremely powerful so speaking of which you have a paper on nuclear fusion uh magnetic control of tokamak plasmas to deep reinforcement learning so you uh you're seeking to solve nuclear fusion with deep rl so it's doing control of high temperature plasmas can you explain this work and uh can ai eventually solve nuclear fusion it's been very fun last year or two and very productive because we've been taking off a lot of my dream projects if you like of things that i've collected over the years of areas of science that i would like to i think could be very transformative if we helped accelerate and uh really interesting problems scientific challenges in of themselves this is energy so energy yes exactly so\\n      Question: is the topic of nuclear fusion discussed in this video? if yes then what was discussed\\n    \")"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxxcV2C_DXqt"
      },
      "source": [
        "## Step 4 - Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX6vxSoUHBok",
        "outputId": "eade1e56-b8af-4b7e-c34b-08a4a1b0da00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, the topic of nuclear fusion is discussed in the video. The discussion includes the following points:\n",
            "\n",
            "1. The speaker mentions a problem in fusion that was published in a Nature paper, where they developed a controller that can hold plasma in specific shapes for a record amount of time, which is crucial for energy production.\n",
            "\n",
            "2. They talk about collaborating with EPFL in Switzerland, which has a test reactor that they used for their experiments. The focus is on identifying bottleneck problems in fusion and applying AI methods to address those challenges.\n",
            "\n",
            "3. The speaker emphasizes the potential of AI to help accelerate solutions in energy and climate, specifically mentioning fusion as an area where AI can contribute.\n",
            "\n",
            "4. They also mention their work on magnetic control of tokamak plasmas using deep reinforcement learning, indicating that they are exploring how AI can assist in controlling high-temperature plasmas for nuclear fusion.\n"
          ]
        }
      ],
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH2Ph0NcDlo5"
      },
      "source": [
        "## Building a Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdTwSS3nHKRz"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGezE1qYQJ76"
      },
      "outputs": [],
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmYnYqbWQWLi"
      },
      "outputs": [],
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGI1hEvfQvLb",
        "outputId": "093b395c-69da-44d8-dbef-b313f3752687"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'context': \"the following is a conversation with demus hasabis ceo and co-founder of deepmind a company that has published and builds some of the most incredible artificial intelligence systems in the history of computing including alfred zero that learned all by itself to play the game of gold better than any human in the world and alpha fold two that solved protein folding both tasks considered nearly impossible for a very long time demus is widely considered to be one of the most brilliant and impactful humans in the history of artificial intelligence and science and engineering in general this was truly an honor and a pleasure for me to finally sit down with him for this conversation and i'm sure we will talk many times again in the future this is the lex friedman podcast to support it please check out our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough\\n\\ndeeper maybe simpler explanation yes of things right than the standard model of physics which we know doesn't work but we still keep adding to so um and and that's how i think the beginning of an explanation would look and it would start encompassing many of the mysteries that we have wondered about for thousands of years like you know consciousness uh life and gravity all of these things yeah giving us a glimpses of explanations for those things yeah well um damas dear one of the special human beings in this giant puzzle of ours and it's a huge honor that you would take a pause from the bigger puzzle to solve this small puzzle of a conversation with me today it's truly an honor and a pleasure thank you thank you i really enjoyed it thanks lex thanks for listening to this conversation with demas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than\\n\\nout our sponsors in the description and now dear friends here's demis hassabis let's start with a bit of a personal question am i an ai program you wrote to interview people until i get good enough to interview you well i'll be impressed if if you were i'd be impressed by myself if you were i don't think we're quite up to that yet but uh maybe you're from the future lex if you did would you tell me is that is that a good thing to tell a language model that's tasked with interviewing that it is in fact um ai maybe we're in a kind of meta turing test uh probably probably it would be a good idea not to tell you so it doesn't change your behavior right this is a kind of heisenberg uncertainty principle situation if i told you you behave differently yeah maybe that's what's happening with us of course this is a benchmark from the future where they replay 2022 as a year before ais were good enough yet and now we want to see is it going to pass exactly if i was such a program would you be\\n\\ndemas establish to support this podcast please check out our sponsors in the description and now let me leave you with some words from edskar dykstra computer science is no more about computers than astronomy is about telescopes thank you for listening and hope to see you next time\",\n",
              " 'question': 'who is Demis'}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parallel_chain.invoke('who is Demis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6osgdBfRCPN"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3e2en89QyOC"
      },
      "outputs": [],
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Ur7Ph_xlRE-7",
        "outputId": "92122b01-36e9-4de4-85cb-6eb1e893a33d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The video features a conversation with Demas, who discusses the need for deeper and simpler explanations in physics, particularly in relation to consciousness, life, and gravity. He emphasizes the limitations of the current standard model of physics and the importance of exploring more fundamental explanations. Additionally, he talks about advancements in fusion research, specifically how they have managed to hold plasma in specific shapes for extended periods, which is a significant step in fusion energy production. The conversation touches on the potential of AI in modeling quantum mechanical behavior, particularly regarding electrons. The discussion concludes with a quote about the nature of computer science.'"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "main_chain.invoke('Can you summarize the video')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyERl2UwRKn6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
